{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural Network models - Leave One Participant Out CV to predict properties using 3 subwindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "#pd.set_option(\"display.max_rows\", None)\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated random seed is 58\n"
     ]
    }
   ],
   "source": [
    "# Initialise the random state\n",
    "#num = random.randint(1, 500)\n",
    "num = 58\n",
    "torch.manual_seed(num)\n",
    "np.random.seed(num)\n",
    "print(f\"The generated random seed is {num}\") #347"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"complete_dataset_\"+str(15)+\"subwindows_\"+str(3)+\"slices.csv\"\n",
    "df = pd.read_csv(path)\n",
    "#data.iloc[12959:12965,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22679, 190)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18899, 190)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove enjoyment as we are only considering physical properties\n",
    "print(df.shape)\n",
    "physical_df = df[df.property_name!='enjoyment']\n",
    "physical_df.reset_index(inplace=True, drop=True)\n",
    "physical_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_data = physical_df.iloc[:10800,:]\n",
    "new_data = physical_df.iloc[10800:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10800, 180)\n",
      "(8099, 180)\n",
      "(10800, 180)\n",
      "(8099, 180)\n"
     ]
    }
   ],
   "source": [
    "starting_index = 10\n",
    "\n",
    "# Create a df of features\n",
    "existing_features_df = existing_data.iloc[:,starting_index:]\n",
    "new_features_df = new_data.iloc[:,starting_index:]\n",
    "\n",
    "# Create a df with the first 10 columns\n",
    "existing_info = existing_data.iloc[:, :starting_index]\n",
    "new_info = new_data.iloc[:, :starting_index]\n",
    "\n",
    "normalised_existing_features = existing_features_df.copy()\n",
    "normalised_new_features = new_features_df.copy()\n",
    "print(normalised_existing_features.shape)\n",
    "print(normalised_new_features.shape)\n",
    "\n",
    "# create scaler\n",
    "scaler = MinMaxScaler(feature_range=(-1,1)) # As this is the range of the activation function - tanh\n",
    "\n",
    "# fit scaler and apply transform\n",
    "normalised_existing_features[normalised_existing_features.columns] = scaler.fit_transform(existing_features_df[existing_features_df.columns])\n",
    "normalised_new_features[normalised_new_features.columns] = scaler.fit_transform(new_features_df[new_features_df.columns])\n",
    "\n",
    "print(normalised_existing_features.shape)\n",
    "print(normalised_new_features.shape)\n",
    "\n",
    "normalised_existing_df = pd.concat([existing_info, normalised_existing_features], axis=1)\n",
    "normalised_new_df = pd.concat([new_info, normalised_new_features], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create X and y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_2d(df, features_starting_idx):\n",
    "    \n",
    "    X_2d = df.iloc[:,features_starting_idx:].values\n",
    "    \n",
    "    X_tensor_2d = torch.Tensor(X_2d)    \n",
    "    return X_tensor_2d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y_train_for_2d_X(df, predicting_feature = 'property_id', output_as_tensor='Yes'):\n",
    "    # CreatE an instance of a one-hot-encoder\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Perform one-hot encoding on the specified column \n",
    "    encoder_df = pd.DataFrame(encoder.fit_transform(df[[predicting_feature]]).toarray())\n",
    "    \n",
    "    # Convert to a numpy array\n",
    "    y_train = encoder_df.to_numpy()\n",
    "    \n",
    "    if output_as_tensor == 'Yes':\n",
    "        # Convert to a tensor\n",
    "        y_train = torch.Tensor(y_train)\n",
    "\n",
    "    return y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y_test_for_2d_X(df, predicting_feature = 'property_id'):   \n",
    "    y_test = df[predicting_feature].values\n",
    "    #if predicting_feature == 'property_id':\n",
    "       # y_test = y_test - 3\n",
    "    if predicting_feature == 'rating_level_num':\n",
    "        y_test = y_test - 1\n",
    "    \n",
    "    y_test_tensor = torch.Tensor(y_test)    \n",
    "    y_test_tensor = y_test_tensor.type(torch.LongTensor)\n",
    "    \n",
    "    return y_test_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_3d(df, features_starting_idx):\n",
    "    dim1 = df.new_interaction_id.nunique()\n",
    "    #print(dim1)\n",
    "    dim2 = df.slice_num.nunique()\n",
    "    dim3 = df.iloc[:,features_starting_idx:].shape[1]\n",
    "        \n",
    "    X = np.zeros((dim1, dim2, dim3)) \n",
    "\n",
    "    itr_id_lst = df.new_interaction_id.unique().tolist()\n",
    "    #print(itr_id_lst[0], itr_id_lst[-1])\n",
    "\n",
    "    for itr_id in itr_id_lst: #range(len(itr_id_lst)):\n",
    "        #itr_id = itr_id_lst[i]\n",
    "        itr_id_df = df[df.new_interaction_id==itr_id]  \n",
    "        \n",
    "        for j in range(itr_id_df.shape[0]):\n",
    "            vals_arr = itr_id_df.iloc[j,features_starting_idx:].values\n",
    "            if itr_id-1 == dim1:\n",
    "                print(itr_id)\n",
    "            X[itr_id-1,j] = vals_arr\n",
    "    \n",
    "    X_tensor = torch.Tensor(X)    \n",
    "    return X_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y_train_for_3d_X(df, predicting_feature = 'property_id'):\n",
    "    # Create a dataset with only the required columns\n",
    "    df2 = df[['new_interaction_id', 'property_id', 'rating_level_num']]\n",
    "\n",
    "    # Remove duplicates\n",
    "    df2.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "    # Reset the indexes\n",
    "    df2.reset_index(drop=True, inplace=True) \n",
    "    \n",
    "    ## Create y train\n",
    "    # CreatE an instance of a one-hot-encoder\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Perform one-hot encoding on the specified column \n",
    "    encoder_df = pd.DataFrame(encoder.fit_transform(df2[[predicting_feature]]).toarray())\n",
    "    \n",
    "    # Convert to a numpy array\n",
    "    y_train = encoder_df.to_numpy()\n",
    "    \n",
    "    # Convert to a tensor\n",
    "    y_train = torch.Tensor(y_train)\n",
    "  \n",
    "    return y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y_test_for_3d_X(df, predicting_feature = 'property_id'):\n",
    "    # Create a dataset with only the required columns\n",
    "    df2 = df[['new_interaction_id', 'property_id', 'rating_level_num']]\n",
    "\n",
    "    # Remove duplicates\n",
    "    df2.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "    # Reset the indexes\n",
    "    df2.reset_index(drop=True, inplace=True) \n",
    "    \n",
    "    y_test = df2[predicting_feature].values\n",
    "    #if predicting_feature == 'property_id':\n",
    "       # y_test = y_test - 3\n",
    "    if predicting_feature == 'rating_level_num':\n",
    "        y_test = y_test - 1\n",
    "    \n",
    "    y_test = torch.Tensor(y_test)    \n",
    "    y_test = y_test.type(torch.LongTensor)\n",
    "    \n",
    "    return y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model 2 - LSTM model using all 180 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_all_features_properties(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(90, 40, 1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(3 * 40 * 2, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 5)\n",
    "          \n",
    "    def forward(self, x1, x2): #, x2\n",
    "        x1, (hn, cn) = self.rnn(x1) #, (self.h0, self.c0)\n",
    "        x1 = F.tanh(x1)\n",
    "        x2, (hm, cm) = self.rnn(x2) # (self.h0, self.c0)\n",
    "        x2 = F.tanh(x2)\n",
    "        x = torch.cat((x1, x2), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_best_model_for_3d_X(train_dataloader, learning_rate, num_epochs, model):\n",
    "\n",
    "    # Model\n",
    "    train_model = model\n",
    "\n",
    "    # Loss and Optimiser\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(train_model.parameters(), lr=learning_rate, momentum=0.7)\n",
    "\n",
    "    best_train_loss = np.inf\n",
    "    best_model = None\n",
    "    #best_model_epoch_num = np.inf\n",
    "    train_loss_lst = []\n",
    "   # val_loss_lst = []\n",
    "   # avg_loss_lst = []\n",
    "\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        #Set the model in training mode\n",
    "        train_model.train()\n",
    "\n",
    "        # Initialise the total training and validation loss\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "        avg_loss = 0\n",
    "\n",
    "        #running_loss = 0.0\n",
    "        for i, train_data in enumerate(train_dataloader, 0):\n",
    "            #print(len(train_data))\n",
    "\n",
    "            # get the inputs; data is a list of [input1, input2, label]\n",
    "            train_input1, train_input2, train_labels = train_data #train_input2, \n",
    "\n",
    "            #train_labels = train_labels.type(torch.LongTensor)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            train_preds = train_model(train_input1, train_input2)  \n",
    "            #print(train_labels)#\n",
    "\n",
    "            train_loss = criterion(train_preds, train_labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update training loss\n",
    "            epoch_train_loss += train_loss.item()\n",
    "\n",
    "     \n",
    "        avg_training_loss = epoch_train_loss / len(train_dataloader) #count_train\n",
    "        \n",
    "        train_loss_lst.append(avg_training_loss)\n",
    "         \n",
    "        #print(f'epoch {epoch+1}: train loss = {round(avg_training_loss,3)}, val loss = {round(avg_validation_loss,3)}, average loss = {round(avg_loss,3)}')\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'epoch {epoch+1}: train loss = {round(avg_training_loss,2)}')\n",
    "        \n",
    "        if avg_training_loss < best_train_loss:\n",
    "            best_train_loss = avg_training_loss\n",
    "            best_model = train_model.state_dict()\n",
    "\n",
    "    return best_train_loss, best_model, train_loss_lst \n",
    "\n",
    "    #return best_avg_loss, best_model, train_loss_lst, val_loss_lst, avg_loss_lst   #, avg_loss_lst, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "def LSTM_LOP0CV(lili_0, lili_1, new_0, new_1, model, num_folds=5, predicting_feature='property_id', learning_rate=0.01, num_epochs=10, random_state=num): #, num_inner_folds=5\n",
    "    # Set fixed random number seed\n",
    "    torch.manual_seed(num)\n",
    "        \n",
    "    total_conf_mat = 0\n",
    "    micro_f1_lst = []\n",
    "    acc_lst = []\n",
    "    if predicting_feature == 'property_id':\n",
    "        macro_f1_lst = []\n",
    "    elif predicting_feature == 'rating_level_num':\n",
    "        weighted_f1_lst = [] \n",
    "        \n",
    "        data0_add = create_y_train_for_2d_X(data_0, predicting_feature = 'property_id', output_as_tensor='No')\n",
    "        data0_add_pd = pd.DataFrame(data0_add, columns = ['smoothness','thickness','warmth', 'flexibility', 'softness'])\n",
    "        data_0 = pd.concat([data_0.reset_index(drop=True), data0_add_pd.reset_index(drop=True)], axis=1)\n",
    "        data1_add = create_y_train_for_2d_X(data_1, predicting_feature = 'property_id', output_as_tensor='No')\n",
    "        data1_add_pd = pd.DataFrame(data1_add, columns = ['smoothness','thickness','warmth', 'flexibility', 'softness'])\n",
    "        data_1 = pd.concat([data_1.reset_index(drop=True), data1_add_pd.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    #for participant in sorted(data_0.participant_id.unique()): # # #:lst: #\n",
    "       # print(f'LEAVING PARTICIPANT {participant} OUT:')\n",
    "        \n",
    "        # Split the data into training and testing\n",
    "        #training_data_0 = data_0[data_0.participant_id != participant] \n",
    "        #training_data_1 = data_1[data_1.participant_id != participant] \n",
    "        #testing_data_0 = data_0[data_0.participant_id == participant] \n",
    "        #testing_data_1 = data_1[data_1.participant_id == participant] \n",
    "\n",
    "    # Data preparation\n",
    "    X_train_0 = create_X_3d(lili_0, 11)\n",
    "    X_train_1 = create_X_3d(lili_1, 11) \n",
    "    X_test_0 = create_X_3d(new_0, 11) \n",
    "    X_test_1 = create_X_3d(new_1, 11)           \n",
    "    y_train = create_y_train_for_3d_X(lili_0, predicting_feature = predicting_feature)\n",
    "    y_test = create_y_test_for_3d_X(new_0, predicting_feature = predicting_feature)        \n",
    "\n",
    "\n",
    "        #print(X_train_0.shape)\n",
    "        #print(X_train_1.shape)\n",
    "        #print(y_train.shape)\n",
    "        #print(X_test_0.shape)\n",
    "        #print(X_test_1.shape)\n",
    "        #print(y_test.shape)\n",
    " \n",
    "    # Create the datasets and dataloaders\n",
    "    train_dataset = TensorDataset(X_train_0, X_train_1, y_train) \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=y_train.shape[0]) # num_workers=2,\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_0, X_test_1, y_test)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, shuffle=True, batch_size=y_test.shape[0]) # num_workers=2,       \n",
    "            \n",
    "    train_loss, best_model, train_loss_lst = find_best_model_for_3d_X(train_dataloader, learning_rate, num_epochs, model)\n",
    "\n",
    "    # save trained model \n",
    "    name = 'best_model.pt'\n",
    "    torch.save(best_model, name)\n",
    "    print(f'The model has been saved')\n",
    "      \n",
    "        \n",
    "    test_model = model\n",
    "    test_model.load_state_dict(torch.load(name))\n",
    "\n",
    "    dataiter = iter(test_dataloader) \n",
    "    test_input1, test_input2, test_labels = dataiter.next() \n",
    "\n",
    "    test_preds = test_model(test_input1, test_input2) \n",
    "\n",
    "    test_preds_np = test_preds.detach().numpy()\n",
    "    test_predicted_np = np.argmax(test_preds_np, axis = 1)\n",
    "\n",
    "    test_labels_np = test_labels.numpy()    \n",
    "    \n",
    "    if predicting_feature == 'property_id':\n",
    "        conf_mat = confusion_matrix(test_labels_np, test_predicted_np, labels=[0, 1, 2, 3, 4])\n",
    "        macro_f1_score = f1_score(test_labels_np, test_predicted_np, average='macro') \n",
    "        #macro_f1_lst.append(macro_f1_score) \n",
    "    elif predicting_feature == 'rating_level_num':\n",
    "        conf_mat = confusion_matrix(test_labels_np, test_predicted_np, labels=[0,1,2])\n",
    "        weighted_f1_score = f1_score(test_labels_np, test_predicted_np, average='weighted') \n",
    "        weighted_f1_lst.append(weighted_f1_score)\n",
    "\n",
    "    #total_conf_mat += conf_mat\n",
    "    micro_f1_score = f1_score(test_labels_np, test_predicted_np, average='micro')  \n",
    "    #micro_f1_lst.append(micro_f1_score)\n",
    "    acc = accuracy_score(test_labels_np, test_predicted_np)\n",
    "    #acc_lst.append(acc)\n",
    "\n",
    "\n",
    "    #print(f\"Leaving participant {participant} out\")\n",
    "    print(\"(1) Confusion matrix:\\n\", conf_mat)\n",
    "    print(f\"(2) micro F1 score = {round(micro_f1_score,2)}\") \n",
    "    if predicting_feature == 'property_id':\n",
    "        print(f\"(3) Macro F1 score = {round(macro_f1_score,2)}\")\n",
    "    elif predicting_feature == 'rating_level_num':\n",
    "        print(f\"(3) Weighted F1 score = {round(weighted_f1_score,2)}\")            \n",
    "    print(f\"(4) Percentage Classification accuracy = {round(acc*100,2)}%\")\n",
    "\n",
    "    print('--------------------------------')\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to interact\n",
    "\n",
    "normalised_existing_df.insert(5, \"new_interaction_id\", None)\n",
    "normalised_existing_df['new_interaction_id'] = normalised_existing_df.groupby(['participant_id', 'clothes_id', 'property_id', 'sub_window_num'], sort=False).ngroup() + 1\n",
    "\n",
    "normalised_new_df.insert(5, \"new_interaction_id\", None)\n",
    "normalised_new_df['new_interaction_id'] = normalised_new_df.groupby(['participant_id', 'clothes_id', 'property_id', 'sub_window_num'], sort=False).ngroup() + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hand1_emg = normalised_existing_df.iloc[:,35:59]\n",
    "\n",
    "a_hand0_acc = normalised_existing_df.iloc[:,59:86]\n",
    "a_hand1_acc = normalised_existing_df.iloc[:,86:113]\n",
    "\n",
    "a_hand0_qua = normalised_existing_df.iloc[:,113:152]\n",
    "a_hand1_qua = normalised_existing_df.iloc[:,152:]\n",
    "\n",
    "a_df_info = normalised_existing_df.iloc[:,:11]\n",
    "\n",
    "# Combine the data to ceate a df for each hand\n",
    "a_emg_0 = normalised_existing_df.iloc[:,:35]\n",
    "#a_emg_1 = pd.concat([df_info, hand1_emg], axis=1)\n",
    "\n",
    "existing_0 = pd.concat([a_emg_0, a_hand0_acc, a_hand0_qua], axis=1)\n",
    "existing_1 = pd.concat([a_df_info, a_hand1_emg, a_hand1_acc, a_hand1_qua], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_hand1_emg = normalised_new_df.iloc[:,35:59]\n",
    "\n",
    "b_hand0_acc = normalised_new_df.iloc[:,59:86]\n",
    "b_hand1_acc = normalised_new_df.iloc[:,86:113]\n",
    "\n",
    "b_hand0_qua = normalised_new_df.iloc[:,113:152]\n",
    "b_hand1_qua = normalised_new_df.iloc[:,152:]\n",
    "\n",
    "b_df_info = normalised_new_df.iloc[:,:11]\n",
    "\n",
    "# Combine the data to ceate a df for each hand\n",
    "b_emg_0 = normalised_new_df.iloc[:,:35]\n",
    "#a_emg_1 = pd.concat([df_info, hand1_emg], axis=1)\n",
    "\n",
    "new_0 = pd.concat([b_emg_0, b_hand0_acc, b_hand0_qua], axis=1)\n",
    "new_1 = pd.concat([b_df_info, b_hand1_emg, b_hand1_acc, b_hand1_qua], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10800, 101)\n",
      "(10800, 101)\n",
      "(8099, 101)\n",
      "(8099, 101)\n"
     ]
    }
   ],
   "source": [
    "print(existing_0.shape)\n",
    "print(existing_1.shape)\n",
    "\n",
    "print(new_0.shape)\n",
    "print(new_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss = 1.61\n",
      "epoch 11: train loss = 1.61\n",
      "epoch 21: train loss = 1.61\n",
      "epoch 31: train loss = 1.61\n",
      "epoch 41: train loss = 1.61\n",
      "epoch 51: train loss = 1.61\n",
      "epoch 61: train loss = 1.61\n",
      "epoch 71: train loss = 1.61\n",
      "epoch 81: train loss = 1.61\n",
      "epoch 91: train loss = 1.6\n",
      "epoch 101: train loss = 1.6\n",
      "epoch 111: train loss = 1.58\n",
      "epoch 121: train loss = 1.64\n",
      "epoch 131: train loss = 1.62\n",
      "epoch 141: train loss = 1.61\n",
      "epoch 151: train loss = 1.58\n",
      "epoch 161: train loss = 1.62\n",
      "epoch 171: train loss = 1.54\n",
      "epoch 181: train loss = 1.51\n",
      "epoch 191: train loss = 1.55\n",
      "epoch 201: train loss = 1.49\n",
      "epoch 211: train loss = 1.56\n",
      "epoch 221: train loss = 1.58\n",
      "epoch 231: train loss = 1.47\n",
      "epoch 241: train loss = 1.5\n",
      "epoch 251: train loss = 1.48\n",
      "epoch 261: train loss = 1.43\n",
      "epoch 271: train loss = 1.53\n",
      "epoch 281: train loss = 1.46\n",
      "epoch 291: train loss = 1.52\n",
      "epoch 301: train loss = 1.47\n",
      "epoch 311: train loss = 1.48\n",
      "epoch 321: train loss = 1.44\n",
      "epoch 331: train loss = 1.46\n",
      "epoch 341: train loss = 1.5\n",
      "epoch 351: train loss = 1.43\n",
      "epoch 361: train loss = 1.47\n",
      "epoch 371: train loss = 1.4\n",
      "epoch 381: train loss = 1.47\n",
      "epoch 391: train loss = 1.49\n",
      "epoch 401: train loss = 1.45\n",
      "epoch 411: train loss = 1.42\n",
      "epoch 421: train loss = 1.42\n",
      "epoch 431: train loss = 1.4\n",
      "epoch 441: train loss = 1.36\n",
      "epoch 451: train loss = 1.37\n",
      "epoch 461: train loss = 1.39\n",
      "epoch 471: train loss = 1.38\n",
      "epoch 481: train loss = 1.33\n",
      "epoch 491: train loss = 1.34\n",
      "epoch 501: train loss = 1.38\n",
      "epoch 511: train loss = 1.4\n",
      "epoch 521: train loss = 1.27\n",
      "epoch 531: train loss = 1.33\n",
      "epoch 541: train loss = 1.29\n",
      "epoch 551: train loss = 1.37\n",
      "epoch 561: train loss = 1.3\n",
      "epoch 571: train loss = 1.29\n",
      "epoch 581: train loss = 1.27\n",
      "epoch 591: train loss = 1.31\n",
      "epoch 601: train loss = 1.26\n",
      "epoch 611: train loss = 1.4\n",
      "epoch 621: train loss = 1.24\n",
      "epoch 631: train loss = 1.25\n",
      "epoch 641: train loss = 1.29\n",
      "epoch 651: train loss = 1.24\n",
      "epoch 661: train loss = 1.22\n",
      "epoch 671: train loss = 1.24\n",
      "epoch 681: train loss = 1.21\n",
      "epoch 691: train loss = 1.17\n",
      "epoch 701: train loss = 1.2\n",
      "epoch 711: train loss = 1.21\n",
      "epoch 721: train loss = 1.25\n",
      "epoch 731: train loss = 1.18\n",
      "epoch 741: train loss = 1.15\n",
      "epoch 751: train loss = 1.16\n",
      "epoch 761: train loss = 1.22\n",
      "epoch 771: train loss = 1.17\n",
      "epoch 781: train loss = 1.14\n",
      "epoch 791: train loss = 1.15\n",
      "epoch 801: train loss = 1.16\n",
      "epoch 811: train loss = 1.14\n",
      "epoch 821: train loss = 1.14\n",
      "epoch 831: train loss = 1.12\n",
      "epoch 841: train loss = 1.15\n",
      "epoch 851: train loss = 1.14\n",
      "epoch 861: train loss = 1.12\n",
      "epoch 871: train loss = 1.13\n",
      "epoch 881: train loss = 1.21\n",
      "epoch 891: train loss = 1.14\n",
      "epoch 901: train loss = 1.16\n",
      "epoch 911: train loss = 1.11\n",
      "epoch 921: train loss = 1.1\n",
      "epoch 931: train loss = 1.13\n",
      "epoch 941: train loss = 1.09\n",
      "epoch 951: train loss = 1.16\n",
      "epoch 961: train loss = 1.11\n",
      "epoch 971: train loss = 1.09\n",
      "epoch 981: train loss = 1.12\n",
      "epoch 991: train loss = 1.09\n",
      "epoch 1001: train loss = 1.08\n",
      "epoch 1011: train loss = 1.08\n",
      "epoch 1021: train loss = 1.09\n",
      "epoch 1031: train loss = 1.1\n",
      "epoch 1041: train loss = 1.07\n",
      "epoch 1051: train loss = 1.08\n",
      "epoch 1061: train loss = 1.08\n",
      "epoch 1071: train loss = 1.1\n",
      "epoch 1081: train loss = 1.07\n",
      "epoch 1091: train loss = 1.12\n",
      "epoch 1101: train loss = 1.09\n",
      "epoch 1111: train loss = 1.06\n",
      "epoch 1121: train loss = 1.08\n",
      "epoch 1131: train loss = 1.08\n",
      "epoch 1141: train loss = 1.1\n",
      "epoch 1151: train loss = 1.06\n",
      "epoch 1161: train loss = 1.05\n",
      "epoch 1171: train loss = 1.05\n",
      "epoch 1181: train loss = 1.03\n",
      "epoch 1191: train loss = 1.06\n",
      "epoch 1201: train loss = 1.08\n",
      "epoch 1211: train loss = 1.05\n",
      "epoch 1221: train loss = 1.05\n",
      "epoch 1231: train loss = 1.05\n",
      "epoch 1241: train loss = 1.07\n",
      "epoch 1251: train loss = 1.06\n",
      "epoch 1261: train loss = 1.06\n",
      "epoch 1271: train loss = 1.05\n",
      "epoch 1281: train loss = 1.03\n",
      "epoch 1291: train loss = 1.03\n",
      "epoch 1301: train loss = 1.04\n",
      "epoch 1311: train loss = 1.02\n",
      "epoch 1321: train loss = 1.03\n",
      "epoch 1331: train loss = 1.05\n",
      "epoch 1341: train loss = 1.05\n",
      "epoch 1351: train loss = 1.02\n",
      "epoch 1361: train loss = 1.06\n",
      "epoch 1371: train loss = 1.03\n",
      "epoch 1381: train loss = 1.02\n",
      "epoch 1391: train loss = 1.02\n",
      "epoch 1401: train loss = 1.02\n",
      "epoch 1411: train loss = 1.02\n",
      "epoch 1421: train loss = 1.02\n",
      "epoch 1431: train loss = 1.02\n",
      "epoch 1441: train loss = 1.04\n",
      "epoch 1451: train loss = 1.05\n",
      "epoch 1461: train loss = 1.02\n",
      "epoch 1471: train loss = 1.01\n",
      "epoch 1481: train loss = 1.01\n",
      "epoch 1491: train loss = 1.07\n",
      "The model has been saved\n",
      "(1) Confusion matrix:\n",
      " [[195  29  53 152 111]\n",
      " [ 85  35  39 230 151]\n",
      " [ 74  13  36 259 158]\n",
      " [ 89  11  10 285 145]\n",
      " [138  16  29 212 145]]\n",
      "(2) micro F1 score = 0.26\n",
      "(3) Macro F1 score = 0.23\n",
      "(4) Percentage Classification accuracy = 25.78%\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LSTM_LOP0CV(lili_0=existing_0, lili_1=existing_1, new_0=new_0, new_1=new_1, model=LSTM_all_features_properties(), num_folds=5, predicting_feature='property_id', learning_rate=0.5, num_epochs=1500, random_state=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
