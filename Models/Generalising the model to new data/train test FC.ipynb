{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural Network models - Leave One Participant Out CV to predict properties using 3 subwindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "#pd.set_option(\"display.max_rows\", None)\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated random seed is 58\n"
     ]
    }
   ],
   "source": [
    "# Initialise the random state\n",
    "#num = random.randint(1, 500)\n",
    "num = 58\n",
    "torch.manual_seed(num)\n",
    "np.random.seed(num)\n",
    "print(f\"The generated random seed is {num}\") #347"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"complete_dataset_\"+str(15)+\"subwindows_\"+str(10)+\"slices.csv\"\n",
    "df = pd.read_csv(path)\n",
    "#data.iloc[12959:12965,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75588, 190)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(62989, 190)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove enjoyment as we are only considering physical properties\n",
    "print(df.shape)\n",
    "physical_df = df[df.property_name!='enjoyment']\n",
    "physical_df.reset_index(inplace=True, drop=True)\n",
    "physical_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_data = physical_df.iloc[:36000,:]\n",
    "new_data = physical_df.iloc[36000:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_ch1_hand0</th>\n",
       "      <th>max_ch2_hand0</th>\n",
       "      <th>max_ch3_hand0</th>\n",
       "      <th>max_ch4_hand0</th>\n",
       "      <th>max_ch5_hand0</th>\n",
       "      <th>max_ch6_hand0</th>\n",
       "      <th>max_ch7_hand0</th>\n",
       "      <th>max_ch8_hand0</th>\n",
       "      <th>mean_ch1_hand0</th>\n",
       "      <th>mean_ch2_hand0</th>\n",
       "      <th>mean_ch3_hand0</th>\n",
       "      <th>mean_ch4_hand0</th>\n",
       "      <th>mean_ch5_hand0</th>\n",
       "      <th>mean_ch6_hand0</th>\n",
       "      <th>mean_ch7_hand0</th>\n",
       "      <th>mean_ch8_hand0</th>\n",
       "      <th>std_ch1_hand0</th>\n",
       "      <th>std_ch2_hand0</th>\n",
       "      <th>std_ch3_hand0</th>\n",
       "      <th>std_ch4_hand0</th>\n",
       "      <th>std_ch5_hand0</th>\n",
       "      <th>std_ch6_hand0</th>\n",
       "      <th>std_ch7_hand0</th>\n",
       "      <th>std_ch8_hand0</th>\n",
       "      <th>max_ch1_hand1</th>\n",
       "      <th>max_ch2_hand1</th>\n",
       "      <th>max_ch3_hand1</th>\n",
       "      <th>max_ch4_hand1</th>\n",
       "      <th>max_ch5_hand1</th>\n",
       "      <th>max_ch6_hand1</th>\n",
       "      <th>max_ch7_hand1</th>\n",
       "      <th>max_ch8_hand1</th>\n",
       "      <th>mean_ch1_hand1</th>\n",
       "      <th>mean_ch2_hand1</th>\n",
       "      <th>mean_ch3_hand1</th>\n",
       "      <th>mean_ch4_hand1</th>\n",
       "      <th>mean_ch5_hand1</th>\n",
       "      <th>mean_ch6_hand1</th>\n",
       "      <th>mean_ch7_hand1</th>\n",
       "      <th>mean_ch8_hand1</th>\n",
       "      <th>std_ch1_hand1</th>\n",
       "      <th>std_ch2_hand1</th>\n",
       "      <th>std_ch3_hand1</th>\n",
       "      <th>std_ch4_hand1</th>\n",
       "      <th>std_ch5_hand1</th>\n",
       "      <th>std_ch6_hand1</th>\n",
       "      <th>std_ch7_hand1</th>\n",
       "      <th>std_ch8_hand1</th>\n",
       "      <th>max_Ax_hand0</th>\n",
       "      <th>max_Ay_hand0</th>\n",
       "      <th>max_Az_hand0</th>\n",
       "      <th>max_Vx_hand0</th>\n",
       "      <th>max_Vy_hand0</th>\n",
       "      <th>max_Vz_hand0</th>\n",
       "      <th>max_Jx_hand0</th>\n",
       "      <th>max_Jy_hand0</th>\n",
       "      <th>max_Jz_hand0</th>\n",
       "      <th>mean_Ax_hand0</th>\n",
       "      <th>mean_Ay_hand0</th>\n",
       "      <th>mean_Az_hand0</th>\n",
       "      <th>mean_Vx_hand0</th>\n",
       "      <th>mean_Vy_hand0</th>\n",
       "      <th>mean_Vz_hand0</th>\n",
       "      <th>mean_Jx_hand0</th>\n",
       "      <th>mean_Jy_hand0</th>\n",
       "      <th>mean_Jz_hand0</th>\n",
       "      <th>std_Ax_hand0</th>\n",
       "      <th>std_Ay_hand0</th>\n",
       "      <th>std_Az_hand0</th>\n",
       "      <th>std_Vx_hand0</th>\n",
       "      <th>std_Vy_hand0</th>\n",
       "      <th>std_Vz_hand0</th>\n",
       "      <th>std_Jx_hand0</th>\n",
       "      <th>std_Jy_hand0</th>\n",
       "      <th>std_Jz_hand0</th>\n",
       "      <th>max_Ax_hand1</th>\n",
       "      <th>max_Ay_hand1</th>\n",
       "      <th>max_Az_hand1</th>\n",
       "      <th>max_Vx_hand1</th>\n",
       "      <th>max_Vy_hand1</th>\n",
       "      <th>max_Vz_hand1</th>\n",
       "      <th>max_Jx_hand1</th>\n",
       "      <th>max_Jy_hand1</th>\n",
       "      <th>max_Jz_hand1</th>\n",
       "      <th>mean_Ax_hand1</th>\n",
       "      <th>mean_Ay_hand1</th>\n",
       "      <th>mean_Az_hand1</th>\n",
       "      <th>mean_Vx_hand1</th>\n",
       "      <th>mean_Vy_hand1</th>\n",
       "      <th>mean_Vz_hand1</th>\n",
       "      <th>mean_Jx_hand1</th>\n",
       "      <th>mean_Jy_hand1</th>\n",
       "      <th>mean_Jz_hand1</th>\n",
       "      <th>std_Ax_hand1</th>\n",
       "      <th>std_Ay_hand1</th>\n",
       "      <th>std_Az_hand1</th>\n",
       "      <th>std_Vx_hand1</th>\n",
       "      <th>std_Vy_hand1</th>\n",
       "      <th>std_Vz_hand1</th>\n",
       "      <th>std_Jx_hand1</th>\n",
       "      <th>std_Jy_hand1</th>\n",
       "      <th>std_Jz_hand1</th>\n",
       "      <th>max_w_hand0</th>\n",
       "      <th>max_x_hand0</th>\n",
       "      <th>max_y_hand0</th>\n",
       "      <th>max_z_hand0</th>\n",
       "      <th>max_AVx_hand0</th>\n",
       "      <th>max_AVy_hand0</th>\n",
       "      <th>max_AVz_hand0</th>\n",
       "      <th>max_AAx_hand0</th>\n",
       "      <th>max_AAy_hand0</th>\n",
       "      <th>max_AAz_hand0</th>\n",
       "      <th>max_AJx_hand0</th>\n",
       "      <th>max_AJy_hand0</th>\n",
       "      <th>max_AJz_hand0</th>\n",
       "      <th>mean_w_hand0</th>\n",
       "      <th>mean_x_hand0</th>\n",
       "      <th>mean_y_hand0</th>\n",
       "      <th>mean_z_hand0</th>\n",
       "      <th>mean_AVx_hand0</th>\n",
       "      <th>mean_AVy_hand0</th>\n",
       "      <th>mean_AVz_hand0</th>\n",
       "      <th>mean_AAx_hand0</th>\n",
       "      <th>mean_AAy_hand0</th>\n",
       "      <th>mean_AAz_hand0</th>\n",
       "      <th>mean_AJx_hand0</th>\n",
       "      <th>mean_AJy_hand0</th>\n",
       "      <th>mean_AJz_hand0</th>\n",
       "      <th>std_w_hand0</th>\n",
       "      <th>std_x_hand0</th>\n",
       "      <th>std_y_hand0</th>\n",
       "      <th>std_z_hand0</th>\n",
       "      <th>std_AVx_hand0</th>\n",
       "      <th>std_AVy_hand0</th>\n",
       "      <th>std_AVz_hand0</th>\n",
       "      <th>std_AAx_hand0</th>\n",
       "      <th>std_AAy_hand0</th>\n",
       "      <th>std_AAz_hand0</th>\n",
       "      <th>std_AJx_hand0</th>\n",
       "      <th>std_AJy_hand0</th>\n",
       "      <th>std_AJz_hand0</th>\n",
       "      <th>max_w_hand1</th>\n",
       "      <th>max_x_hand1</th>\n",
       "      <th>max_y_hand1</th>\n",
       "      <th>max_z_hand1</th>\n",
       "      <th>max_AVx_hand1</th>\n",
       "      <th>max_AVy_hand1</th>\n",
       "      <th>max_AVz_hand1</th>\n",
       "      <th>max_AAx_hand1</th>\n",
       "      <th>max_AAy_hand1</th>\n",
       "      <th>max_AAz_hand1</th>\n",
       "      <th>max_AJx_hand1</th>\n",
       "      <th>max_AJy_hand1</th>\n",
       "      <th>max_AJz_hand1</th>\n",
       "      <th>mean_w_hand1</th>\n",
       "      <th>mean_x_hand1</th>\n",
       "      <th>mean_y_hand1</th>\n",
       "      <th>mean_z_hand1</th>\n",
       "      <th>mean_AVx_hand1</th>\n",
       "      <th>mean_AVy_hand1</th>\n",
       "      <th>mean_AVz_hand1</th>\n",
       "      <th>mean_AAx_hand1</th>\n",
       "      <th>mean_AAy_hand1</th>\n",
       "      <th>mean_AAz_hand1</th>\n",
       "      <th>mean_AJx_hand1</th>\n",
       "      <th>mean_AJy_hand1</th>\n",
       "      <th>mean_AJz_hand1</th>\n",
       "      <th>std_w_hand1</th>\n",
       "      <th>std_x_hand1</th>\n",
       "      <th>std_y_hand1</th>\n",
       "      <th>std_z_hand1</th>\n",
       "      <th>std_AVx_hand1</th>\n",
       "      <th>std_AVy_hand1</th>\n",
       "      <th>std_AVz_hand1</th>\n",
       "      <th>std_AAx_hand1</th>\n",
       "      <th>std_AAy_hand1</th>\n",
       "      <th>std_AAz_hand1</th>\n",
       "      <th>std_AJx_hand1</th>\n",
       "      <th>std_AJy_hand1</th>\n",
       "      <th>std_AJz_hand1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.670391</td>\n",
       "      <td>0.495868</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.652406</td>\n",
       "      <td>0.75625</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.497805</td>\n",
       "      <td>0.613462</td>\n",
       "      <td>0.665764</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>0.718656</td>\n",
       "      <td>0.636197</td>\n",
       "      <td>0.748926</td>\n",
       "      <td>0.660851</td>\n",
       "      <td>0.002324</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.004039</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.908451</td>\n",
       "      <td>0.751445</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.493878</td>\n",
       "      <td>0.995918</td>\n",
       "      <td>0.648515</td>\n",
       "      <td>0.576037</td>\n",
       "      <td>0.496670</td>\n",
       "      <td>0.846831</td>\n",
       "      <td>0.691595</td>\n",
       "      <td>0.636244</td>\n",
       "      <td>0.491412</td>\n",
       "      <td>0.608674</td>\n",
       "      <td>0.601382</td>\n",
       "      <td>0.558852</td>\n",
       "      <td>0.004198</td>\n",
       "      <td>0.014491</td>\n",
       "      <td>0.015432</td>\n",
       "      <td>0.026432</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.218816</td>\n",
       "      <td>0.022186</td>\n",
       "      <td>0.006429</td>\n",
       "      <td>-0.857056</td>\n",
       "      <td>-0.084885</td>\n",
       "      <td>0.479568</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.718061</td>\n",
       "      <td>0.488281</td>\n",
       "      <td>1.206342</td>\n",
       "      <td>-0.863110</td>\n",
       "      <td>-0.091721</td>\n",
       "      <td>0.468826</td>\n",
       "      <td>9.082031e-06</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.036854</td>\n",
       "      <td>0.039343</td>\n",
       "      <td>0.092439</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.007619</td>\n",
       "      <td>0.010341</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.488563</td>\n",
       "      <td>0.441082</td>\n",
       "      <td>0.750762</td>\n",
       "      <td>0.986160</td>\n",
       "      <td>0.013489</td>\n",
       "      <td>-0.211105</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>1.448006</td>\n",
       "      <td>0.708008</td>\n",
       "      <td>2.343750</td>\n",
       "      <td>0.964774</td>\n",
       "      <td>-0.007117</td>\n",
       "      <td>-0.249875</td>\n",
       "      <td>1.117188e-04</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.104780</td>\n",
       "      <td>0.096063</td>\n",
       "      <td>0.335256</td>\n",
       "      <td>0.022364</td>\n",
       "      <td>0.013963</td>\n",
       "      <td>0.028878</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>1.260192</td>\n",
       "      <td>0.571902</td>\n",
       "      <td>1.565671</td>\n",
       "      <td>0.179352</td>\n",
       "      <td>0.484946</td>\n",
       "      <td>0.160615</td>\n",
       "      <td>-0.840857</td>\n",
       "      <td>0.009831</td>\n",
       "      <td>0.012959</td>\n",
       "      <td>0.117029</td>\n",
       "      <td>1.270143</td>\n",
       "      <td>2.238883</td>\n",
       "      <td>19.812834</td>\n",
       "      <td>154.113175</td>\n",
       "      <td>104.818227</td>\n",
       "      <td>1338.775861</td>\n",
       "      <td>0.178765</td>\n",
       "      <td>0.484513</td>\n",
       "      <td>0.160469</td>\n",
       "      <td>-0.841154</td>\n",
       "      <td>-0.016003</td>\n",
       "      <td>-0.027618</td>\n",
       "      <td>-0.081836</td>\n",
       "      <td>0.129868</td>\n",
       "      <td>0.552890</td>\n",
       "      <td>-1.016133</td>\n",
       "      <td>16.779031</td>\n",
       "      <td>-45.915024</td>\n",
       "      <td>210.907738</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.016423</td>\n",
       "      <td>0.031903</td>\n",
       "      <td>0.200440</td>\n",
       "      <td>1.160057</td>\n",
       "      <td>1.546528</td>\n",
       "      <td>13.337295</td>\n",
       "      <td>97.220032</td>\n",
       "      <td>155.178181</td>\n",
       "      <td>1114.885458</td>\n",
       "      <td>0.600343</td>\n",
       "      <td>-0.175433</td>\n",
       "      <td>-0.769104</td>\n",
       "      <td>-0.129347</td>\n",
       "      <td>0.140072</td>\n",
       "      <td>0.440644</td>\n",
       "      <td>0.126734</td>\n",
       "      <td>5.745156</td>\n",
       "      <td>20.612016</td>\n",
       "      <td>5.331893</td>\n",
       "      <td>212.817696</td>\n",
       "      <td>779.034203</td>\n",
       "      <td>579.105295</td>\n",
       "      <td>0.598260</td>\n",
       "      <td>-0.176827</td>\n",
       "      <td>-0.770695</td>\n",
       "      <td>-0.129768</td>\n",
       "      <td>0.053843</td>\n",
       "      <td>0.125821</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>0.487977</td>\n",
       "      <td>0.515185</td>\n",
       "      <td>-1.737315</td>\n",
       "      <td>-44.056190</td>\n",
       "      <td>-91.541428</td>\n",
       "      <td>81.034379</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.073437</td>\n",
       "      <td>0.280630</td>\n",
       "      <td>0.115024</td>\n",
       "      <td>3.958058</td>\n",
       "      <td>13.903979</td>\n",
       "      <td>6.202001</td>\n",
       "      <td>170.597999</td>\n",
       "      <td>792.769940</td>\n",
       "      <td>449.786951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.670391</td>\n",
       "      <td>0.495868</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.641711</td>\n",
       "      <td>0.75625</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.497331</td>\n",
       "      <td>0.612607</td>\n",
       "      <td>0.665735</td>\n",
       "      <td>0.492597</td>\n",
       "      <td>0.717746</td>\n",
       "      <td>0.635807</td>\n",
       "      <td>0.749609</td>\n",
       "      <td>0.661458</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>0.008698</td>\n",
       "      <td>0.005318</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.504098</td>\n",
       "      <td>0.929578</td>\n",
       "      <td>0.757225</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.497959</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.633663</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.497375</td>\n",
       "      <td>0.849582</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.635417</td>\n",
       "      <td>0.491965</td>\n",
       "      <td>0.503827</td>\n",
       "      <td>0.596070</td>\n",
       "      <td>0.557604</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>0.013122</td>\n",
       "      <td>0.013631</td>\n",
       "      <td>0.022183</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>0.111327</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.004858</td>\n",
       "      <td>-0.857056</td>\n",
       "      <td>-0.084885</td>\n",
       "      <td>0.474686</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.563401</td>\n",
       "      <td>0.944010</td>\n",
       "      <td>1.878005</td>\n",
       "      <td>-0.864478</td>\n",
       "      <td>-0.095041</td>\n",
       "      <td>0.466776</td>\n",
       "      <td>9.765625e-08</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>0.069561</td>\n",
       "      <td>-0.096004</td>\n",
       "      <td>0.077474</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.009589</td>\n",
       "      <td>0.009925</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.402547</td>\n",
       "      <td>1.013362</td>\n",
       "      <td>1.039248</td>\n",
       "      <td>0.992020</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>-0.233078</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>1.437717</td>\n",
       "      <td>1.255580</td>\n",
       "      <td>1.489258</td>\n",
       "      <td>0.974197</td>\n",
       "      <td>-0.014669</td>\n",
       "      <td>-0.249680</td>\n",
       "      <td>-1.163737e-05</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.015346</td>\n",
       "      <td>-0.161628</td>\n",
       "      <td>0.199220</td>\n",
       "      <td>0.013709</td>\n",
       "      <td>0.010763</td>\n",
       "      <td>0.014762</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.806361</td>\n",
       "      <td>1.094488</td>\n",
       "      <td>1.251700</td>\n",
       "      <td>0.176189</td>\n",
       "      <td>0.485587</td>\n",
       "      <td>0.159029</td>\n",
       "      <td>-0.841541</td>\n",
       "      <td>0.133141</td>\n",
       "      <td>0.061469</td>\n",
       "      <td>0.043901</td>\n",
       "      <td>4.332320</td>\n",
       "      <td>2.887801</td>\n",
       "      <td>12.346117</td>\n",
       "      <td>165.242446</td>\n",
       "      <td>140.565259</td>\n",
       "      <td>615.469160</td>\n",
       "      <td>0.175885</td>\n",
       "      <td>0.485399</td>\n",
       "      <td>0.158387</td>\n",
       "      <td>-0.841644</td>\n",
       "      <td>0.050176</td>\n",
       "      <td>0.020933</td>\n",
       "      <td>-0.022867</td>\n",
       "      <td>0.428492</td>\n",
       "      <td>0.224097</td>\n",
       "      <td>3.416566</td>\n",
       "      <td>-216.183273</td>\n",
       "      <td>-80.686083</td>\n",
       "      <td>232.653044</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.063817</td>\n",
       "      <td>0.036748</td>\n",
       "      <td>0.077514</td>\n",
       "      <td>3.233995</td>\n",
       "      <td>1.553890</td>\n",
       "      <td>5.736241</td>\n",
       "      <td>586.244076</td>\n",
       "      <td>234.136553</td>\n",
       "      <td>556.915683</td>\n",
       "      <td>0.603038</td>\n",
       "      <td>-0.175326</td>\n",
       "      <td>-0.766516</td>\n",
       "      <td>-0.128901</td>\n",
       "      <td>0.115731</td>\n",
       "      <td>0.321439</td>\n",
       "      <td>0.121293</td>\n",
       "      <td>5.012583</td>\n",
       "      <td>12.815405</td>\n",
       "      <td>7.555850</td>\n",
       "      <td>224.865107</td>\n",
       "      <td>706.861321</td>\n",
       "      <td>549.917517</td>\n",
       "      <td>0.600378</td>\n",
       "      <td>-0.176324</td>\n",
       "      <td>-0.769133</td>\n",
       "      <td>-0.129948</td>\n",
       "      <td>0.030569</td>\n",
       "      <td>0.132933</td>\n",
       "      <td>-0.023579</td>\n",
       "      <td>1.005235</td>\n",
       "      <td>2.467654</td>\n",
       "      <td>-1.775671</td>\n",
       "      <td>2.892800</td>\n",
       "      <td>-87.699311</td>\n",
       "      <td>-108.119283</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.055740</td>\n",
       "      <td>0.140247</td>\n",
       "      <td>0.158323</td>\n",
       "      <td>2.578049</td>\n",
       "      <td>8.649329</td>\n",
       "      <td>8.466336</td>\n",
       "      <td>187.474756</td>\n",
       "      <td>696.353488</td>\n",
       "      <td>622.765618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>0.670391</td>\n",
       "      <td>0.495868</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.75625</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.497504</td>\n",
       "      <td>0.612607</td>\n",
       "      <td>0.666201</td>\n",
       "      <td>0.492511</td>\n",
       "      <td>0.718373</td>\n",
       "      <td>0.636809</td>\n",
       "      <td>0.748307</td>\n",
       "      <td>0.660995</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>0.004658</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.002685</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.699422</td>\n",
       "      <td>0.645503</td>\n",
       "      <td>0.493878</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.618812</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.497268</td>\n",
       "      <td>0.847711</td>\n",
       "      <td>0.691595</td>\n",
       "      <td>0.634039</td>\n",
       "      <td>0.491327</td>\n",
       "      <td>0.476105</td>\n",
       "      <td>0.594884</td>\n",
       "      <td>0.557988</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.092711</td>\n",
       "      <td>0.008191</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>-0.852173</td>\n",
       "      <td>-0.082443</td>\n",
       "      <td>0.475174</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.906808</td>\n",
       "      <td>0.449219</td>\n",
       "      <td>0.592913</td>\n",
       "      <td>-0.862817</td>\n",
       "      <td>-0.094943</td>\n",
       "      <td>0.466385</td>\n",
       "      <td>-1.132812e-05</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.069983</td>\n",
       "      <td>0.019696</td>\n",
       "      <td>-0.033961</td>\n",
       "      <td>0.010145</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.628876</td>\n",
       "      <td>0.446873</td>\n",
       "      <td>0.478738</td>\n",
       "      <td>0.991043</td>\n",
       "      <td>-0.003113</td>\n",
       "      <td>-0.198410</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.785496</td>\n",
       "      <td>0.806726</td>\n",
       "      <td>1.159668</td>\n",
       "      <td>0.972586</td>\n",
       "      <td>-0.017859</td>\n",
       "      <td>-0.217062</td>\n",
       "      <td>-4.882813e-07</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.211373</td>\n",
       "      <td>-0.105115</td>\n",
       "      <td>-0.080995</td>\n",
       "      <td>0.014115</td>\n",
       "      <td>0.009521</td>\n",
       "      <td>0.014976</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.589524</td>\n",
       "      <td>1.040070</td>\n",
       "      <td>0.992292</td>\n",
       "      <td>0.175420</td>\n",
       "      <td>0.486364</td>\n",
       "      <td>0.159025</td>\n",
       "      <td>-0.841592</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.059080</td>\n",
       "      <td>2.017495</td>\n",
       "      <td>1.433494</td>\n",
       "      <td>8.710260</td>\n",
       "      <td>554.574503</td>\n",
       "      <td>268.044509</td>\n",
       "      <td>424.425134</td>\n",
       "      <td>0.174108</td>\n",
       "      <td>0.486077</td>\n",
       "      <td>0.157710</td>\n",
       "      <td>-0.841750</td>\n",
       "      <td>-0.032439</td>\n",
       "      <td>-0.014993</td>\n",
       "      <td>-0.048357</td>\n",
       "      <td>-1.806178</td>\n",
       "      <td>-0.672885</td>\n",
       "      <td>-1.057210</td>\n",
       "      <td>49.814640</td>\n",
       "      <td>3.745668</td>\n",
       "      <td>-139.450471</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.022678</td>\n",
       "      <td>0.016039</td>\n",
       "      <td>0.110523</td>\n",
       "      <td>4.044649</td>\n",
       "      <td>2.649872</td>\n",
       "      <td>7.347502</td>\n",
       "      <td>311.488508</td>\n",
       "      <td>208.452524</td>\n",
       "      <td>736.223136</td>\n",
       "      <td>0.613327</td>\n",
       "      <td>-0.176585</td>\n",
       "      <td>-0.758505</td>\n",
       "      <td>-0.131125</td>\n",
       "      <td>0.139470</td>\n",
       "      <td>0.398959</td>\n",
       "      <td>0.045814</td>\n",
       "      <td>4.164473</td>\n",
       "      <td>12.761640</td>\n",
       "      <td>10.404938</td>\n",
       "      <td>649.805789</td>\n",
       "      <td>1527.420437</td>\n",
       "      <td>519.653553</td>\n",
       "      <td>0.610256</td>\n",
       "      <td>-0.176983</td>\n",
       "      <td>-0.760867</td>\n",
       "      <td>-0.131678</td>\n",
       "      <td>0.051381</td>\n",
       "      <td>0.248855</td>\n",
       "      <td>0.023004</td>\n",
       "      <td>-0.510481</td>\n",
       "      <td>-0.382400</td>\n",
       "      <td>1.409711</td>\n",
       "      <td>64.171485</td>\n",
       "      <td>18.288726</td>\n",
       "      <td>-53.628672</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.085341</td>\n",
       "      <td>0.122248</td>\n",
       "      <td>0.020458</td>\n",
       "      <td>4.730311</td>\n",
       "      <td>9.167239</td>\n",
       "      <td>5.244792</td>\n",
       "      <td>380.644859</td>\n",
       "      <td>894.676534</td>\n",
       "      <td>492.696123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.675978</td>\n",
       "      <td>0.495868</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.76250</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.497676</td>\n",
       "      <td>0.612607</td>\n",
       "      <td>0.666201</td>\n",
       "      <td>0.492166</td>\n",
       "      <td>0.718373</td>\n",
       "      <td>0.637478</td>\n",
       "      <td>0.749349</td>\n",
       "      <td>0.660417</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>0.003366</td>\n",
       "      <td>0.504098</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.705202</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.493878</td>\n",
       "      <td>0.787755</td>\n",
       "      <td>0.628713</td>\n",
       "      <td>0.576037</td>\n",
       "      <td>0.497183</td>\n",
       "      <td>0.848591</td>\n",
       "      <td>0.691835</td>\n",
       "      <td>0.633764</td>\n",
       "      <td>0.492092</td>\n",
       "      <td>0.540434</td>\n",
       "      <td>0.600093</td>\n",
       "      <td>0.559476</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>0.005657</td>\n",
       "      <td>0.004265</td>\n",
       "      <td>0.006547</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>0.109162</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.006607</td>\n",
       "      <td>-0.857056</td>\n",
       "      <td>-0.068771</td>\n",
       "      <td>0.482498</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>1.269531</td>\n",
       "      <td>0.719572</td>\n",
       "      <td>0.770970</td>\n",
       "      <td>-0.866528</td>\n",
       "      <td>-0.084103</td>\n",
       "      <td>0.476443</td>\n",
       "      <td>-6.630859e-05</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>-0.025835</td>\n",
       "      <td>-0.170118</td>\n",
       "      <td>0.186197</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>0.011025</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.777207</td>\n",
       "      <td>0.568810</td>\n",
       "      <td>0.501869</td>\n",
       "      <td>0.974930</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>-0.178879</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>1.185826</td>\n",
       "      <td>1.139323</td>\n",
       "      <td>1.302083</td>\n",
       "      <td>0.964350</td>\n",
       "      <td>-0.008809</td>\n",
       "      <td>-0.191086</td>\n",
       "      <td>-2.908529e-04</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>-0.162318</td>\n",
       "      <td>-0.022144</td>\n",
       "      <td>0.332164</td>\n",
       "      <td>0.012866</td>\n",
       "      <td>0.013375</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>1.224161</td>\n",
       "      <td>1.199073</td>\n",
       "      <td>1.068872</td>\n",
       "      <td>0.174454</td>\n",
       "      <td>0.486522</td>\n",
       "      <td>0.156278</td>\n",
       "      <td>-0.841936</td>\n",
       "      <td>0.006764</td>\n",
       "      <td>0.005302</td>\n",
       "      <td>0.042930</td>\n",
       "      <td>2.314447</td>\n",
       "      <td>0.774435</td>\n",
       "      <td>11.379475</td>\n",
       "      <td>115.560261</td>\n",
       "      <td>65.426803</td>\n",
       "      <td>662.709244</td>\n",
       "      <td>0.172737</td>\n",
       "      <td>0.486404</td>\n",
       "      <td>0.154664</td>\n",
       "      <td>-0.842407</td>\n",
       "      <td>-0.027498</td>\n",
       "      <td>-0.001420</td>\n",
       "      <td>-0.110571</td>\n",
       "      <td>0.531482</td>\n",
       "      <td>0.106488</td>\n",
       "      <td>-0.267177</td>\n",
       "      <td>-34.985147</td>\n",
       "      <td>14.486963</td>\n",
       "      <td>-186.836490</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.036174</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.144953</td>\n",
       "      <td>1.566252</td>\n",
       "      <td>0.466043</td>\n",
       "      <td>9.341831</td>\n",
       "      <td>160.556495</td>\n",
       "      <td>37.410321</td>\n",
       "      <td>921.217386</td>\n",
       "      <td>0.618680</td>\n",
       "      <td>-0.177124</td>\n",
       "      <td>-0.753378</td>\n",
       "      <td>-0.131839</td>\n",
       "      <td>0.173872</td>\n",
       "      <td>0.320532</td>\n",
       "      <td>-0.069988</td>\n",
       "      <td>2.608770</td>\n",
       "      <td>1.143752</td>\n",
       "      <td>-1.314365</td>\n",
       "      <td>-33.100067</td>\n",
       "      <td>3.814586</td>\n",
       "      <td>208.877911</td>\n",
       "      <td>0.617947</td>\n",
       "      <td>-0.177514</td>\n",
       "      <td>-0.754276</td>\n",
       "      <td>-0.133027</td>\n",
       "      <td>0.088206</td>\n",
       "      <td>0.149616</td>\n",
       "      <td>-0.131999</td>\n",
       "      <td>-3.232508</td>\n",
       "      <td>-7.028403</td>\n",
       "      <td>-2.625410</td>\n",
       "      <td>-252.208344</td>\n",
       "      <td>-303.849649</td>\n",
       "      <td>-8.786893</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>0.115058</td>\n",
       "      <td>0.195463</td>\n",
       "      <td>0.054990</td>\n",
       "      <td>6.219711</td>\n",
       "      <td>7.714367</td>\n",
       "      <td>2.122456</td>\n",
       "      <td>193.986947</td>\n",
       "      <td>290.840360</td>\n",
       "      <td>216.525244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.670391</td>\n",
       "      <td>0.495868</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.76250</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.497418</td>\n",
       "      <td>0.612286</td>\n",
       "      <td>0.665502</td>\n",
       "      <td>0.491822</td>\n",
       "      <td>0.719126</td>\n",
       "      <td>0.636810</td>\n",
       "      <td>0.748958</td>\n",
       "      <td>0.660532</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.002966</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.004340</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.693642</td>\n",
       "      <td>0.640212</td>\n",
       "      <td>0.493878</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.608911</td>\n",
       "      <td>0.566820</td>\n",
       "      <td>0.496542</td>\n",
       "      <td>0.848371</td>\n",
       "      <td>0.691113</td>\n",
       "      <td>0.634260</td>\n",
       "      <td>0.491008</td>\n",
       "      <td>0.493941</td>\n",
       "      <td>0.594910</td>\n",
       "      <td>0.557604</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>0.004341</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.098174</td>\n",
       "      <td>0.009725</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>-0.857056</td>\n",
       "      <td>-0.075119</td>\n",
       "      <td>0.490311</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.830078</td>\n",
       "      <td>0.600962</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>-0.863013</td>\n",
       "      <td>-0.090549</td>\n",
       "      <td>0.474100</td>\n",
       "      <td>3.515625e-06</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>0.139281</td>\n",
       "      <td>0.167780</td>\n",
       "      <td>-0.097263</td>\n",
       "      <td>0.005067</td>\n",
       "      <td>0.009197</td>\n",
       "      <td>0.012057</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.446644</td>\n",
       "      <td>0.262082</td>\n",
       "      <td>0.830701</td>\n",
       "      <td>1.023270</td>\n",
       "      <td>0.007141</td>\n",
       "      <td>-0.189621</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>4.089355</td>\n",
       "      <td>4.516602</td>\n",
       "      <td>1.617432</td>\n",
       "      <td>0.979603</td>\n",
       "      <td>-0.017552</td>\n",
       "      <td>-0.231474</td>\n",
       "      <td>-6.821987e-05</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>0.569523</td>\n",
       "      <td>0.594643</td>\n",
       "      <td>-1.278555</td>\n",
       "      <td>0.024091</td>\n",
       "      <td>0.018184</td>\n",
       "      <td>0.046005</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>2.398816</td>\n",
       "      <td>1.957802</td>\n",
       "      <td>3.178717</td>\n",
       "      <td>0.171828</td>\n",
       "      <td>0.486377</td>\n",
       "      <td>0.155308</td>\n",
       "      <td>-0.842687</td>\n",
       "      <td>0.051942</td>\n",
       "      <td>0.046418</td>\n",
       "      <td>0.028305</td>\n",
       "      <td>2.167587</td>\n",
       "      <td>1.173944</td>\n",
       "      <td>3.115225</td>\n",
       "      <td>116.744319</td>\n",
       "      <td>42.480494</td>\n",
       "      <td>240.661919</td>\n",
       "      <td>0.171385</td>\n",
       "      <td>0.486033</td>\n",
       "      <td>0.154898</td>\n",
       "      <td>-0.842855</td>\n",
       "      <td>0.023378</td>\n",
       "      <td>0.019177</td>\n",
       "      <td>-0.001750</td>\n",
       "      <td>-0.178144</td>\n",
       "      <td>-0.205674</td>\n",
       "      <td>0.458694</td>\n",
       "      <td>-12.632382</td>\n",
       "      <td>-27.099915</td>\n",
       "      <td>64.146291</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.022904</td>\n",
       "      <td>0.020641</td>\n",
       "      <td>0.022090</td>\n",
       "      <td>1.303469</td>\n",
       "      <td>1.026663</td>\n",
       "      <td>1.524111</td>\n",
       "      <td>108.642753</td>\n",
       "      <td>66.800589</td>\n",
       "      <td>151.705323</td>\n",
       "      <td>0.617756</td>\n",
       "      <td>-0.179027</td>\n",
       "      <td>-0.753498</td>\n",
       "      <td>-0.133178</td>\n",
       "      <td>0.081987</td>\n",
       "      <td>0.076177</td>\n",
       "      <td>0.275227</td>\n",
       "      <td>21.603607</td>\n",
       "      <td>56.450019</td>\n",
       "      <td>29.424453</td>\n",
       "      <td>2797.763022</td>\n",
       "      <td>9050.317251</td>\n",
       "      <td>2366.953044</td>\n",
       "      <td>0.608768</td>\n",
       "      <td>-0.179851</td>\n",
       "      <td>-0.760911</td>\n",
       "      <td>-0.134045</td>\n",
       "      <td>-0.157557</td>\n",
       "      <td>-0.563161</td>\n",
       "      <td>0.016789</td>\n",
       "      <td>0.834538</td>\n",
       "      <td>2.442042</td>\n",
       "      <td>2.794393</td>\n",
       "      <td>357.828889</td>\n",
       "      <td>1043.185698</td>\n",
       "      <td>84.447301</td>\n",
       "      <td>0.008409</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.006875</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.572710</td>\n",
       "      <td>0.179101</td>\n",
       "      <td>12.984182</td>\n",
       "      <td>39.396991</td>\n",
       "      <td>14.842166</td>\n",
       "      <td>1229.137650</td>\n",
       "      <td>3678.905761</td>\n",
       "      <td>1628.943160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_ch1_hand0  max_ch2_hand0  max_ch3_hand0  max_ch4_hand0  max_ch5_hand0  \\\n",
       "0            0.5       0.615385       0.670391       0.495868       0.759036   \n",
       "1            0.5       0.615385       0.670391       0.495868       0.734940   \n",
       "2            0.5       0.620513       0.670391       0.495868       0.740964   \n",
       "3            0.5       0.615385       0.675978       0.495868       0.722892   \n",
       "4            0.5       0.615385       0.670391       0.495868       0.734940   \n",
       "\n",
       "   max_ch6_hand0  max_ch7_hand0  max_ch8_hand0  mean_ch1_hand0  \\\n",
       "0       0.652406        0.75625       0.666667        0.497805   \n",
       "1       0.641711        0.75625       0.666667        0.497331   \n",
       "2       0.647059        0.75625       0.666667        0.497504   \n",
       "3       0.647059        0.76250       0.666667        0.497676   \n",
       "4       0.647059        0.76250       0.666667        0.497418   \n",
       "\n",
       "   mean_ch2_hand0  mean_ch3_hand0  mean_ch4_hand0  mean_ch5_hand0  \\\n",
       "0        0.613462        0.665764        0.492188        0.718656   \n",
       "1        0.612607        0.665735        0.492597        0.717746   \n",
       "2        0.612607        0.666201        0.492511        0.718373   \n",
       "3        0.612607        0.666201        0.492166        0.718373   \n",
       "4        0.612286        0.665502        0.491822        0.719126   \n",
       "\n",
       "   mean_ch6_hand0  mean_ch7_hand0  mean_ch8_hand0  std_ch1_hand0  \\\n",
       "0        0.636197        0.748926        0.660851       0.002324   \n",
       "1        0.635807        0.749609        0.661458       0.002484   \n",
       "2        0.636809        0.748307        0.660995       0.002042   \n",
       "3        0.637478        0.749349        0.660417       0.002544   \n",
       "4        0.636810        0.748958        0.660532       0.002503   \n",
       "\n",
       "   std_ch2_hand0  std_ch3_hand0  std_ch4_hand0  std_ch5_hand0  std_ch6_hand0  \\\n",
       "0       0.002503       0.003077       0.002455       0.006501       0.004039   \n",
       "1       0.002791       0.003327       0.002405       0.008698       0.005318   \n",
       "2       0.002791       0.002938       0.002199       0.007508       0.004658   \n",
       "3       0.002583       0.002703       0.002129       0.003623       0.003481   \n",
       "4       0.002747       0.002966       0.001806       0.004051       0.003087   \n",
       "\n",
       "   std_ch7_hand0  std_ch8_hand0  max_ch1_hand1  max_ch2_hand1  max_ch3_hand1  \\\n",
       "0       0.003061       0.002083       0.508197       0.908451       0.751445   \n",
       "1       0.003954       0.002901       0.504098       0.929578       0.757225   \n",
       "2       0.003347       0.002685       0.500000       0.866197       0.699422   \n",
       "3       0.003469       0.003366       0.504098       0.866197       0.705202   \n",
       "4       0.004340       0.002863       0.500000       0.859155       0.693642   \n",
       "\n",
       "   max_ch4_hand1  max_ch5_hand1  max_ch6_hand1  max_ch7_hand1  max_ch8_hand1  \\\n",
       "0       0.730159       0.493878       0.995918       0.648515       0.576037   \n",
       "1       0.714286       0.497959       0.734694       0.633663       0.571429   \n",
       "2       0.645503       0.493878       0.685714       0.618812       0.571429   \n",
       "3       0.650794       0.493878       0.787755       0.628713       0.576037   \n",
       "4       0.640212       0.493878       0.673469       0.608911       0.566820   \n",
       "\n",
       "   mean_ch1_hand1  mean_ch2_hand1  mean_ch3_hand1  mean_ch4_hand1  \\\n",
       "0        0.496670        0.846831        0.691595        0.636244   \n",
       "1        0.497375        0.849582        0.692467        0.635417   \n",
       "2        0.497268        0.847711        0.691595        0.634039   \n",
       "3        0.497183        0.848591        0.691835        0.633764   \n",
       "4        0.496542        0.848371        0.691113        0.634260   \n",
       "\n",
       "   mean_ch5_hand1  mean_ch6_hand1  mean_ch7_hand1  mean_ch8_hand1  \\\n",
       "0        0.491412        0.608674        0.601382        0.558852   \n",
       "1        0.491965        0.503827        0.596070        0.557604   \n",
       "2        0.491327        0.476105        0.594884        0.557988   \n",
       "3        0.492092        0.540434        0.600093        0.559476   \n",
       "4        0.491008        0.493941        0.594910        0.557604   \n",
       "\n",
       "   std_ch1_hand1  std_ch2_hand1  std_ch3_hand1  std_ch4_hand1  std_ch5_hand1  \\\n",
       "0       0.004198       0.014491       0.015432       0.026432       0.002186   \n",
       "1       0.002954       0.013122       0.013631       0.022183       0.002619   \n",
       "2       0.003087       0.005556       0.003264       0.004274       0.001997   \n",
       "3       0.002839       0.005657       0.004265       0.006547       0.002057   \n",
       "4       0.002553       0.004341       0.002891       0.003333       0.002143   \n",
       "\n",
       "   std_ch6_hand1  std_ch7_hand1  std_ch8_hand1  max_Ax_hand0  max_Ay_hand0  \\\n",
       "0       0.218816       0.022186       0.006429     -0.857056     -0.084885   \n",
       "1       0.111327       0.012712       0.004858     -0.857056     -0.084885   \n",
       "2       0.092711       0.008191       0.005364     -0.852173     -0.082443   \n",
       "3       0.109162       0.014100       0.006607     -0.857056     -0.068771   \n",
       "4       0.098174       0.009725       0.005128     -0.857056     -0.075119   \n",
       "\n",
       "   max_Az_hand0  max_Vx_hand0  max_Vy_hand0  max_Vz_hand0  max_Jx_hand0  \\\n",
       "0      0.479568      0.000208      0.000215      0.000349      0.718061   \n",
       "1      0.474686      0.000095      0.000256      0.000317      0.563401   \n",
       "2      0.475174      0.000330      0.000281      0.000220      0.906808   \n",
       "3      0.482498      0.000286      0.000260      0.000278      1.269531   \n",
       "4      0.490311      0.000216      0.000406      0.000343      0.830078   \n",
       "\n",
       "   max_Jy_hand0  max_Jz_hand0  mean_Ax_hand0  mean_Ay_hand0  mean_Az_hand0  \\\n",
       "0      0.488281      1.206342      -0.863110      -0.091721       0.468826   \n",
       "1      0.944010      1.878005      -0.864478      -0.095041       0.466776   \n",
       "2      0.449219      0.592913      -0.862817      -0.094943       0.466385   \n",
       "3      0.719572      0.770970      -0.866528      -0.084103       0.476443   \n",
       "4      0.600962      0.781250      -0.863013      -0.090549       0.474100   \n",
       "\n",
       "   mean_Vx_hand0  mean_Vy_hand0  mean_Vz_hand0  mean_Jx_hand0  mean_Jy_hand0  \\\n",
       "0   9.082031e-06      -0.000028      -0.000006       0.036854       0.039343   \n",
       "1   9.765625e-08      -0.000016      -0.000067       0.069561      -0.096004   \n",
       "2  -1.132812e-05       0.000098      -0.000050       0.069983       0.019696   \n",
       "3  -6.630859e-05      -0.000054       0.000054      -0.025835      -0.170118   \n",
       "4   3.515625e-06       0.000115      -0.000121       0.139281       0.167780   \n",
       "\n",
       "   mean_Jz_hand0  std_Ax_hand0  std_Ay_hand0  std_Az_hand0  std_Vx_hand0  \\\n",
       "0       0.092439      0.006753      0.007619      0.010341      0.000193   \n",
       "1       0.077474      0.004900      0.009589      0.009925      0.000092   \n",
       "2      -0.033961      0.010145      0.009021      0.006050      0.000275   \n",
       "3       0.186197      0.006789      0.011025      0.006789      0.000228   \n",
       "4      -0.097263      0.005067      0.009197      0.012057      0.000156   \n",
       "\n",
       "   std_Vy_hand0  std_Vz_hand0  std_Jx_hand0  std_Jy_hand0  std_Jz_hand0  \\\n",
       "0      0.000245      0.000281      0.488563      0.441082      0.750762   \n",
       "1      0.000279      0.000253      0.402547      1.013362      1.039248   \n",
       "2      0.000182      0.000209      0.628876      0.446873      0.478738   \n",
       "3      0.000194      0.000224      0.777207      0.568810      0.501869   \n",
       "4      0.000176      0.000394      0.446644      0.262082      0.830701   \n",
       "\n",
       "   max_Ax_hand1  max_Ay_hand1  max_Az_hand1  max_Vx_hand1  max_Vy_hand1  \\\n",
       "0      0.986160      0.013489     -0.211105      0.001218      0.000510   \n",
       "1      0.992020      0.002258     -0.233078      0.000466      0.000281   \n",
       "2      0.991043     -0.003113     -0.198410      0.000416      0.000427   \n",
       "3      0.974930      0.005676     -0.178879      0.000523      0.000256   \n",
       "4      1.023270      0.007141     -0.189621      0.000690      0.000289   \n",
       "\n",
       "   max_Vz_hand1  max_Jx_hand1  max_Jy_hand1  max_Jz_hand1  mean_Ax_hand1  \\\n",
       "0      0.000937      1.448006      0.708008      2.343750       0.964774   \n",
       "1      0.000830      1.437717      1.255580      1.489258       0.974197   \n",
       "2      0.001187      0.785496      0.806726      1.159668       0.972586   \n",
       "3      0.001220      1.185826      1.139323      1.302083       0.964350   \n",
       "4      0.000414      4.089355      4.516602      1.617432       0.979603   \n",
       "\n",
       "   mean_Ay_hand1  mean_Az_hand1  mean_Vx_hand1  mean_Vy_hand1  mean_Vz_hand1  \\\n",
       "0      -0.007117      -0.249875   1.117188e-04       0.000070       0.000027   \n",
       "1      -0.014669      -0.249680  -1.163737e-05      -0.000108       0.000176   \n",
       "2      -0.017859      -0.217062  -4.882813e-07      -0.000010       0.000148   \n",
       "3      -0.008809      -0.191086  -2.908529e-04      -0.000059       0.000385   \n",
       "4      -0.017552      -0.231474  -6.821987e-05       0.000013      -0.000099   \n",
       "\n",
       "   mean_Jx_hand1  mean_Jy_hand1  mean_Jz_hand1  std_Ax_hand1  std_Ay_hand1  \\\n",
       "0      -0.104780       0.096063       0.335256      0.022364      0.013963   \n",
       "1       0.015346      -0.161628       0.199220      0.013709      0.010763   \n",
       "2       0.211373      -0.105115      -0.080995      0.014115      0.009521   \n",
       "3      -0.162318      -0.022144       0.332164      0.012866      0.013375   \n",
       "4       0.569523       0.594643      -1.278555      0.024091      0.018184   \n",
       "\n",
       "   std_Az_hand1  std_Vx_hand1  std_Vy_hand1  std_Vz_hand1  std_Jx_hand1  \\\n",
       "0      0.028878      0.000738      0.000331      0.000943      1.260192   \n",
       "1      0.014762      0.000277      0.000409      0.000450      0.806361   \n",
       "2      0.014976      0.000383      0.000359      0.000618      0.589524   \n",
       "3      0.010642      0.000824      0.000434      0.000794      1.224161   \n",
       "4      0.046005      0.000512      0.000253      0.000316      2.398816   \n",
       "\n",
       "   std_Jy_hand1  std_Jz_hand1  max_w_hand0  max_x_hand0  max_y_hand0  \\\n",
       "0      0.571902      1.565671     0.179352     0.484946     0.160615   \n",
       "1      1.094488      1.251700     0.176189     0.485587     0.159029   \n",
       "2      1.040070      0.992292     0.175420     0.486364     0.159025   \n",
       "3      1.199073      1.068872     0.174454     0.486522     0.156278   \n",
       "4      1.957802      3.178717     0.171828     0.486377     0.155308   \n",
       "\n",
       "   max_z_hand0  max_AVx_hand0  max_AVy_hand0  max_AVz_hand0  max_AAx_hand0  \\\n",
       "0    -0.840857       0.009831       0.012959       0.117029       1.270143   \n",
       "1    -0.841541       0.133141       0.061469       0.043901       4.332320   \n",
       "2    -0.841592       0.004300       0.006697       0.059080       2.017495   \n",
       "3    -0.841936       0.006764       0.005302       0.042930       2.314447   \n",
       "4    -0.842687       0.051942       0.046418       0.028305       2.167587   \n",
       "\n",
       "   max_AAy_hand0  max_AAz_hand0  max_AJx_hand0  max_AJy_hand0  max_AJz_hand0  \\\n",
       "0       2.238883      19.812834     154.113175     104.818227    1338.775861   \n",
       "1       2.887801      12.346117     165.242446     140.565259     615.469160   \n",
       "2       1.433494       8.710260     554.574503     268.044509     424.425134   \n",
       "3       0.774435      11.379475     115.560261      65.426803     662.709244   \n",
       "4       1.173944       3.115225     116.744319      42.480494     240.661919   \n",
       "\n",
       "   mean_w_hand0  mean_x_hand0  mean_y_hand0  mean_z_hand0  mean_AVx_hand0  \\\n",
       "0      0.178765      0.484513      0.160469     -0.841154       -0.016003   \n",
       "1      0.175885      0.485399      0.158387     -0.841644        0.050176   \n",
       "2      0.174108      0.486077      0.157710     -0.841750       -0.032439   \n",
       "3      0.172737      0.486404      0.154664     -0.842407       -0.027498   \n",
       "4      0.171385      0.486033      0.154898     -0.842855        0.023378   \n",
       "\n",
       "   mean_AVy_hand0  mean_AVz_hand0  mean_AAx_hand0  mean_AAy_hand0  \\\n",
       "0       -0.027618       -0.081836        0.129868        0.552890   \n",
       "1        0.020933       -0.022867        0.428492        0.224097   \n",
       "2       -0.014993       -0.048357       -1.806178       -0.672885   \n",
       "3       -0.001420       -0.110571        0.531482        0.106488   \n",
       "4        0.019177       -0.001750       -0.178144       -0.205674   \n",
       "\n",
       "   mean_AAz_hand0  mean_AJx_hand0  mean_AJy_hand0  mean_AJz_hand0  \\\n",
       "0       -1.016133       16.779031      -45.915024      210.907738   \n",
       "1        3.416566     -216.183273      -80.686083      232.653044   \n",
       "2       -1.057210       49.814640        3.745668     -139.450471   \n",
       "3       -0.267177      -34.985147       14.486963     -186.836490   \n",
       "4        0.458694      -12.632382      -27.099915       64.146291   \n",
       "\n",
       "   std_w_hand0  std_x_hand0  std_y_hand0  std_z_hand0  std_AVx_hand0  \\\n",
       "0     0.000451     0.000472     0.000210     0.000335       0.016423   \n",
       "1     0.000331     0.000123     0.000423     0.000076       0.063817   \n",
       "2     0.000775     0.000376     0.000817     0.000121       0.022678   \n",
       "3     0.001145     0.000209     0.001076     0.000314       0.036174   \n",
       "4     0.000234     0.000225     0.000440     0.000096       0.022904   \n",
       "\n",
       "   std_AVy_hand0  std_AVz_hand0  std_AAx_hand0  std_AAy_hand0  std_AAz_hand0  \\\n",
       "0       0.031903       0.200440       1.160057       1.546528      13.337295   \n",
       "1       0.036748       0.077514       3.233995       1.553890       5.736241   \n",
       "2       0.016039       0.110523       4.044649       2.649872       7.347502   \n",
       "3       0.004493       0.144953       1.566252       0.466043       9.341831   \n",
       "4       0.020641       0.022090       1.303469       1.026663       1.524111   \n",
       "\n",
       "   std_AJx_hand0  std_AJy_hand0  std_AJz_hand0  max_w_hand1  max_x_hand1  \\\n",
       "0      97.220032     155.178181    1114.885458     0.600343    -0.175433   \n",
       "1     586.244076     234.136553     556.915683     0.603038    -0.175326   \n",
       "2     311.488508     208.452524     736.223136     0.613327    -0.176585   \n",
       "3     160.556495      37.410321     921.217386     0.618680    -0.177124   \n",
       "4     108.642753      66.800589     151.705323     0.617756    -0.179027   \n",
       "\n",
       "   max_y_hand1  max_z_hand1  max_AVx_hand1  max_AVy_hand1  max_AVz_hand1  \\\n",
       "0    -0.769104    -0.129347       0.140072       0.440644       0.126734   \n",
       "1    -0.766516    -0.128901       0.115731       0.321439       0.121293   \n",
       "2    -0.758505    -0.131125       0.139470       0.398959       0.045814   \n",
       "3    -0.753378    -0.131839       0.173872       0.320532      -0.069988   \n",
       "4    -0.753498    -0.133178       0.081987       0.076177       0.275227   \n",
       "\n",
       "   max_AAx_hand1  max_AAy_hand1  max_AAz_hand1  max_AJx_hand1  max_AJy_hand1  \\\n",
       "0       5.745156      20.612016       5.331893     212.817696     779.034203   \n",
       "1       5.012583      12.815405       7.555850     224.865107     706.861321   \n",
       "2       4.164473      12.761640      10.404938     649.805789    1527.420437   \n",
       "3       2.608770       1.143752      -1.314365     -33.100067       3.814586   \n",
       "4      21.603607      56.450019      29.424453    2797.763022    9050.317251   \n",
       "\n",
       "   max_AJz_hand1  mean_w_hand1  mean_x_hand1  mean_y_hand1  mean_z_hand1  \\\n",
       "0     579.105295      0.598260     -0.176827     -0.770695     -0.129768   \n",
       "1     549.917517      0.600378     -0.176324     -0.769133     -0.129948   \n",
       "2     519.653553      0.610256     -0.176983     -0.760867     -0.131678   \n",
       "3     208.877911      0.617947     -0.177514     -0.754276     -0.133027   \n",
       "4    2366.953044      0.608768     -0.179851     -0.760911     -0.134045   \n",
       "\n",
       "   mean_AVx_hand1  mean_AVy_hand1  mean_AVz_hand1  mean_AAx_hand1  \\\n",
       "0        0.053843        0.125821       -0.043329        0.487977   \n",
       "1        0.030569        0.132933       -0.023579        1.005235   \n",
       "2        0.051381        0.248855        0.023004       -0.510481   \n",
       "3        0.088206        0.149616       -0.131999       -3.232508   \n",
       "4       -0.157557       -0.563161        0.016789        0.834538   \n",
       "\n",
       "   mean_AAy_hand1  mean_AAz_hand1  mean_AJx_hand1  mean_AJy_hand1  \\\n",
       "0        0.515185       -1.737315      -44.056190      -91.541428   \n",
       "1        2.467654       -1.775671        2.892800      -87.699311   \n",
       "2       -0.382400        1.409711       64.171485       18.288726   \n",
       "3       -7.028403       -2.625410     -252.208344     -303.849649   \n",
       "4        2.442042        2.794393      357.828889     1043.185698   \n",
       "\n",
       "   mean_AJz_hand1  std_w_hand1  std_x_hand1  std_y_hand1  std_z_hand1  \\\n",
       "0       81.034379     0.002302     0.000906     0.001676     0.000510   \n",
       "1     -108.119283     0.001406     0.000820     0.001312     0.000867   \n",
       "2      -53.628672     0.002365     0.000324     0.001801     0.000421   \n",
       "3       -8.786893     0.001179     0.000635     0.001239     0.001194   \n",
       "4       84.447301     0.008409     0.000700     0.006875     0.000894   \n",
       "\n",
       "   std_AVx_hand1  std_AVy_hand1  std_AVz_hand1  std_AAx_hand1  std_AAy_hand1  \\\n",
       "0       0.073437       0.280630       0.115024       3.958058      13.903979   \n",
       "1       0.055740       0.140247       0.158323       2.578049       8.649329   \n",
       "2       0.085341       0.122248       0.020458       4.730311       9.167239   \n",
       "3       0.115058       0.195463       0.054990       6.219711       7.714367   \n",
       "4       0.167600       0.572710       0.179101      12.984182      39.396991   \n",
       "\n",
       "   std_AAz_hand1  std_AJx_hand1  std_AJy_hand1  std_AJz_hand1  \n",
       "0       6.202001     170.597999     792.769940     449.786951  \n",
       "1       8.466336     187.474756     696.353488     622.765618  \n",
       "2       5.244792     380.644859     894.676534     492.696123  \n",
       "3       2.122456     193.986947     290.840360     216.525244  \n",
       "4      14.842166    1229.137650    3678.905761    1628.943160  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_index = 10\n",
    "# Obtain a df of features\n",
    "features_df = physical_df.iloc[:,starting_index:]\n",
    "features_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62989, 180)\n"
     ]
    }
   ],
   "source": [
    "normalised_features_df = features_df.copy()\n",
    "print(normalised_features_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62989, 180)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create scaler\n",
    "scaler = MinMaxScaler(feature_range=(-1,1)) # As this is the range of the activation function - tanh\n",
    "\n",
    "# fit scaler and apply transform\n",
    "normalised_features_df[normalised_features_df.columns] = scaler.fit_transform(features_df[features_df.columns])\n",
    "normalised_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>clothes_id</th>\n",
       "      <th>property_id</th>\n",
       "      <th>property_name</th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_level</th>\n",
       "      <th>rating_level_num</th>\n",
       "      <th>sub_window_num</th>\n",
       "      <th>slice_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>smoothness</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>smoothness</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>smoothness</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>smoothness</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>smoothness</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant_id  clothes_id  property_id property_name  interaction_id  \\\n",
       "0               7          14            0    smoothness               1   \n",
       "1               7          14            0    smoothness               1   \n",
       "2               7          14            0    smoothness               1   \n",
       "3               7          14            0    smoothness               1   \n",
       "4               7          14            0    smoothness               1   \n",
       "\n",
       "   rating rating_level  rating_level_num  sub_window_num  slice_num  \n",
       "0       3       medium                 2               1          1  \n",
       "1       3       medium                 2               1          2  \n",
       "2       3       medium                 2               1          3  \n",
       "3       3       medium                 2               1          4  \n",
       "4       3       medium                 2               1          5  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info = physical_df.iloc[:, :starting_index]\n",
    "df_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62989, 190)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalised_df = pd.concat([df_info,normalised_features_df], axis=1)\n",
    "normalised_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_lili = normalised_df.iloc[:10800,:]\n",
    "normalised_dat_2022 = normalised_df.iloc[10800:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>clothes_id</th>\n",
       "      <th>property_id</th>\n",
       "      <th>property_name</th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_level</th>\n",
       "      <th>rating_level_num</th>\n",
       "      <th>sub_window_num</th>\n",
       "      <th>slice_num</th>\n",
       "      <th>max_ch1_hand0</th>\n",
       "      <th>max_ch2_hand0</th>\n",
       "      <th>max_ch3_hand0</th>\n",
       "      <th>max_ch4_hand0</th>\n",
       "      <th>max_ch5_hand0</th>\n",
       "      <th>max_ch6_hand0</th>\n",
       "      <th>max_ch7_hand0</th>\n",
       "      <th>max_ch8_hand0</th>\n",
       "      <th>mean_ch1_hand0</th>\n",
       "      <th>mean_ch2_hand0</th>\n",
       "      <th>mean_ch3_hand0</th>\n",
       "      <th>mean_ch4_hand0</th>\n",
       "      <th>mean_ch5_hand0</th>\n",
       "      <th>mean_ch6_hand0</th>\n",
       "      <th>mean_ch7_hand0</th>\n",
       "      <th>mean_ch8_hand0</th>\n",
       "      <th>std_ch1_hand0</th>\n",
       "      <th>std_ch2_hand0</th>\n",
       "      <th>std_ch3_hand0</th>\n",
       "      <th>std_ch4_hand0</th>\n",
       "      <th>std_ch5_hand0</th>\n",
       "      <th>std_ch6_hand0</th>\n",
       "      <th>std_ch7_hand0</th>\n",
       "      <th>std_ch8_hand0</th>\n",
       "      <th>max_ch1_hand1</th>\n",
       "      <th>max_ch2_hand1</th>\n",
       "      <th>max_ch3_hand1</th>\n",
       "      <th>max_ch4_hand1</th>\n",
       "      <th>max_ch5_hand1</th>\n",
       "      <th>max_ch6_hand1</th>\n",
       "      <th>max_ch7_hand1</th>\n",
       "      <th>max_ch8_hand1</th>\n",
       "      <th>mean_ch1_hand1</th>\n",
       "      <th>mean_ch2_hand1</th>\n",
       "      <th>mean_ch3_hand1</th>\n",
       "      <th>mean_ch4_hand1</th>\n",
       "      <th>mean_ch5_hand1</th>\n",
       "      <th>mean_ch6_hand1</th>\n",
       "      <th>mean_ch7_hand1</th>\n",
       "      <th>mean_ch8_hand1</th>\n",
       "      <th>std_ch1_hand1</th>\n",
       "      <th>std_ch2_hand1</th>\n",
       "      <th>std_ch3_hand1</th>\n",
       "      <th>std_ch4_hand1</th>\n",
       "      <th>std_ch5_hand1</th>\n",
       "      <th>std_ch6_hand1</th>\n",
       "      <th>std_ch7_hand1</th>\n",
       "      <th>std_ch8_hand1</th>\n",
       "      <th>max_Ax_hand0</th>\n",
       "      <th>max_Ay_hand0</th>\n",
       "      <th>max_Az_hand0</th>\n",
       "      <th>max_Vx_hand0</th>\n",
       "      <th>max_Vy_hand0</th>\n",
       "      <th>max_Vz_hand0</th>\n",
       "      <th>max_Jx_hand0</th>\n",
       "      <th>max_Jy_hand0</th>\n",
       "      <th>max_Jz_hand0</th>\n",
       "      <th>mean_Ax_hand0</th>\n",
       "      <th>mean_Ay_hand0</th>\n",
       "      <th>mean_Az_hand0</th>\n",
       "      <th>mean_Vx_hand0</th>\n",
       "      <th>mean_Vy_hand0</th>\n",
       "      <th>mean_Vz_hand0</th>\n",
       "      <th>mean_Jx_hand0</th>\n",
       "      <th>mean_Jy_hand0</th>\n",
       "      <th>mean_Jz_hand0</th>\n",
       "      <th>std_Ax_hand0</th>\n",
       "      <th>std_Ay_hand0</th>\n",
       "      <th>std_Az_hand0</th>\n",
       "      <th>std_Vx_hand0</th>\n",
       "      <th>std_Vy_hand0</th>\n",
       "      <th>std_Vz_hand0</th>\n",
       "      <th>std_Jx_hand0</th>\n",
       "      <th>std_Jy_hand0</th>\n",
       "      <th>std_Jz_hand0</th>\n",
       "      <th>max_Ax_hand1</th>\n",
       "      <th>max_Ay_hand1</th>\n",
       "      <th>max_Az_hand1</th>\n",
       "      <th>max_Vx_hand1</th>\n",
       "      <th>max_Vy_hand1</th>\n",
       "      <th>max_Vz_hand1</th>\n",
       "      <th>max_Jx_hand1</th>\n",
       "      <th>max_Jy_hand1</th>\n",
       "      <th>max_Jz_hand1</th>\n",
       "      <th>mean_Ax_hand1</th>\n",
       "      <th>mean_Ay_hand1</th>\n",
       "      <th>mean_Az_hand1</th>\n",
       "      <th>mean_Vx_hand1</th>\n",
       "      <th>mean_Vy_hand1</th>\n",
       "      <th>mean_Vz_hand1</th>\n",
       "      <th>mean_Jx_hand1</th>\n",
       "      <th>mean_Jy_hand1</th>\n",
       "      <th>mean_Jz_hand1</th>\n",
       "      <th>std_Ax_hand1</th>\n",
       "      <th>std_Ay_hand1</th>\n",
       "      <th>std_Az_hand1</th>\n",
       "      <th>std_Vx_hand1</th>\n",
       "      <th>std_Vy_hand1</th>\n",
       "      <th>std_Vz_hand1</th>\n",
       "      <th>std_Jx_hand1</th>\n",
       "      <th>std_Jy_hand1</th>\n",
       "      <th>std_Jz_hand1</th>\n",
       "      <th>max_w_hand0</th>\n",
       "      <th>max_x_hand0</th>\n",
       "      <th>max_y_hand0</th>\n",
       "      <th>max_z_hand0</th>\n",
       "      <th>max_AVx_hand0</th>\n",
       "      <th>max_AVy_hand0</th>\n",
       "      <th>max_AVz_hand0</th>\n",
       "      <th>max_AAx_hand0</th>\n",
       "      <th>max_AAy_hand0</th>\n",
       "      <th>max_AAz_hand0</th>\n",
       "      <th>max_AJx_hand0</th>\n",
       "      <th>max_AJy_hand0</th>\n",
       "      <th>max_AJz_hand0</th>\n",
       "      <th>mean_w_hand0</th>\n",
       "      <th>mean_x_hand0</th>\n",
       "      <th>mean_y_hand0</th>\n",
       "      <th>mean_z_hand0</th>\n",
       "      <th>mean_AVx_hand0</th>\n",
       "      <th>mean_AVy_hand0</th>\n",
       "      <th>mean_AVz_hand0</th>\n",
       "      <th>mean_AAx_hand0</th>\n",
       "      <th>mean_AAy_hand0</th>\n",
       "      <th>mean_AAz_hand0</th>\n",
       "      <th>mean_AJx_hand0</th>\n",
       "      <th>mean_AJy_hand0</th>\n",
       "      <th>mean_AJz_hand0</th>\n",
       "      <th>std_w_hand0</th>\n",
       "      <th>std_x_hand0</th>\n",
       "      <th>std_y_hand0</th>\n",
       "      <th>std_z_hand0</th>\n",
       "      <th>std_AVx_hand0</th>\n",
       "      <th>std_AVy_hand0</th>\n",
       "      <th>std_AVz_hand0</th>\n",
       "      <th>std_AAx_hand0</th>\n",
       "      <th>std_AAy_hand0</th>\n",
       "      <th>std_AAz_hand0</th>\n",
       "      <th>std_AJx_hand0</th>\n",
       "      <th>std_AJy_hand0</th>\n",
       "      <th>std_AJz_hand0</th>\n",
       "      <th>max_w_hand1</th>\n",
       "      <th>max_x_hand1</th>\n",
       "      <th>max_y_hand1</th>\n",
       "      <th>max_z_hand1</th>\n",
       "      <th>max_AVx_hand1</th>\n",
       "      <th>max_AVy_hand1</th>\n",
       "      <th>max_AVz_hand1</th>\n",
       "      <th>max_AAx_hand1</th>\n",
       "      <th>max_AAy_hand1</th>\n",
       "      <th>max_AAz_hand1</th>\n",
       "      <th>max_AJx_hand1</th>\n",
       "      <th>max_AJy_hand1</th>\n",
       "      <th>max_AJz_hand1</th>\n",
       "      <th>mean_w_hand1</th>\n",
       "      <th>mean_x_hand1</th>\n",
       "      <th>mean_y_hand1</th>\n",
       "      <th>mean_z_hand1</th>\n",
       "      <th>mean_AVx_hand1</th>\n",
       "      <th>mean_AVy_hand1</th>\n",
       "      <th>mean_AVz_hand1</th>\n",
       "      <th>mean_AAx_hand1</th>\n",
       "      <th>mean_AAy_hand1</th>\n",
       "      <th>mean_AAz_hand1</th>\n",
       "      <th>mean_AJx_hand1</th>\n",
       "      <th>mean_AJy_hand1</th>\n",
       "      <th>mean_AJz_hand1</th>\n",
       "      <th>std_w_hand1</th>\n",
       "      <th>std_x_hand1</th>\n",
       "      <th>std_y_hand1</th>\n",
       "      <th>std_z_hand1</th>\n",
       "      <th>std_AVx_hand1</th>\n",
       "      <th>std_AVy_hand1</th>\n",
       "      <th>std_AVz_hand1</th>\n",
       "      <th>std_AAx_hand1</th>\n",
       "      <th>std_AAy_hand1</th>\n",
       "      <th>std_AAz_hand1</th>\n",
       "      <th>std_AJx_hand1</th>\n",
       "      <th>std_AJy_hand1</th>\n",
       "      <th>std_AJz_hand1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>smoothness</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.670391</td>\n",
       "      <td>0.495868</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.652406</td>\n",
       "      <td>0.75625</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.497805</td>\n",
       "      <td>0.613462</td>\n",
       "      <td>0.665764</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>0.718656</td>\n",
       "      <td>0.636197</td>\n",
       "      <td>0.748926</td>\n",
       "      <td>0.660851</td>\n",
       "      <td>0.002324</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.004039</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.908451</td>\n",
       "      <td>0.751445</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.493878</td>\n",
       "      <td>0.995918</td>\n",
       "      <td>0.648515</td>\n",
       "      <td>0.576037</td>\n",
       "      <td>0.496670</td>\n",
       "      <td>0.846831</td>\n",
       "      <td>0.691595</td>\n",
       "      <td>0.636244</td>\n",
       "      <td>0.491412</td>\n",
       "      <td>0.608674</td>\n",
       "      <td>0.601382</td>\n",
       "      <td>0.558852</td>\n",
       "      <td>0.004198</td>\n",
       "      <td>0.014491</td>\n",
       "      <td>0.015432</td>\n",
       "      <td>0.026432</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.218816</td>\n",
       "      <td>0.022186</td>\n",
       "      <td>0.006429</td>\n",
       "      <td>-0.857056</td>\n",
       "      <td>-0.084885</td>\n",
       "      <td>0.479568</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.718061</td>\n",
       "      <td>0.488281</td>\n",
       "      <td>1.206342</td>\n",
       "      <td>-0.863110</td>\n",
       "      <td>-0.091721</td>\n",
       "      <td>0.468826</td>\n",
       "      <td>9.082031e-06</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.036854</td>\n",
       "      <td>0.039343</td>\n",
       "      <td>0.092439</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.007619</td>\n",
       "      <td>0.010341</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.488563</td>\n",
       "      <td>0.441082</td>\n",
       "      <td>0.750762</td>\n",
       "      <td>0.98616</td>\n",
       "      <td>0.013489</td>\n",
       "      <td>-0.211105</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>1.448006</td>\n",
       "      <td>0.708008</td>\n",
       "      <td>2.343750</td>\n",
       "      <td>0.964774</td>\n",
       "      <td>-0.007117</td>\n",
       "      <td>-0.249875</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.104780</td>\n",
       "      <td>0.096063</td>\n",
       "      <td>0.335256</td>\n",
       "      <td>0.022364</td>\n",
       "      <td>0.013963</td>\n",
       "      <td>0.028878</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>1.260192</td>\n",
       "      <td>0.571902</td>\n",
       "      <td>1.565671</td>\n",
       "      <td>0.179352</td>\n",
       "      <td>0.484946</td>\n",
       "      <td>0.160615</td>\n",
       "      <td>-0.840857</td>\n",
       "      <td>0.009831</td>\n",
       "      <td>0.012959</td>\n",
       "      <td>0.117029</td>\n",
       "      <td>1.270143</td>\n",
       "      <td>2.238883</td>\n",
       "      <td>19.812834</td>\n",
       "      <td>154.113175</td>\n",
       "      <td>104.818227</td>\n",
       "      <td>1338.775861</td>\n",
       "      <td>0.178765</td>\n",
       "      <td>0.484513</td>\n",
       "      <td>0.160469</td>\n",
       "      <td>-0.841154</td>\n",
       "      <td>-0.016003</td>\n",
       "      <td>-0.027618</td>\n",
       "      <td>-0.081836</td>\n",
       "      <td>0.129868</td>\n",
       "      <td>0.552890</td>\n",
       "      <td>-1.016133</td>\n",
       "      <td>16.779031</td>\n",
       "      <td>-45.915024</td>\n",
       "      <td>210.907738</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.016423</td>\n",
       "      <td>0.031903</td>\n",
       "      <td>0.200440</td>\n",
       "      <td>1.160057</td>\n",
       "      <td>1.546528</td>\n",
       "      <td>13.337295</td>\n",
       "      <td>97.220032</td>\n",
       "      <td>155.178181</td>\n",
       "      <td>1114.885458</td>\n",
       "      <td>0.600343</td>\n",
       "      <td>-0.175433</td>\n",
       "      <td>-0.769104</td>\n",
       "      <td>-0.129347</td>\n",
       "      <td>0.140072</td>\n",
       "      <td>0.440644</td>\n",
       "      <td>0.126734</td>\n",
       "      <td>5.745156</td>\n",
       "      <td>20.612016</td>\n",
       "      <td>5.331893</td>\n",
       "      <td>212.817696</td>\n",
       "      <td>779.034203</td>\n",
       "      <td>579.105295</td>\n",
       "      <td>0.598260</td>\n",
       "      <td>-0.176827</td>\n",
       "      <td>-0.770695</td>\n",
       "      <td>-0.129768</td>\n",
       "      <td>0.053843</td>\n",
       "      <td>0.125821</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>0.487977</td>\n",
       "      <td>0.515185</td>\n",
       "      <td>-1.737315</td>\n",
       "      <td>-44.05619</td>\n",
       "      <td>-91.541428</td>\n",
       "      <td>81.034379</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.073437</td>\n",
       "      <td>0.280630</td>\n",
       "      <td>0.115024</td>\n",
       "      <td>3.958058</td>\n",
       "      <td>13.903979</td>\n",
       "      <td>6.202001</td>\n",
       "      <td>170.597999</td>\n",
       "      <td>792.769940</td>\n",
       "      <td>449.786951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>smoothness</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.670391</td>\n",
       "      <td>0.495868</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.641711</td>\n",
       "      <td>0.75625</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.497331</td>\n",
       "      <td>0.612607</td>\n",
       "      <td>0.665735</td>\n",
       "      <td>0.492597</td>\n",
       "      <td>0.717746</td>\n",
       "      <td>0.635807</td>\n",
       "      <td>0.749609</td>\n",
       "      <td>0.661458</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>0.008698</td>\n",
       "      <td>0.005318</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.504098</td>\n",
       "      <td>0.929578</td>\n",
       "      <td>0.757225</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.497959</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.633663</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.497375</td>\n",
       "      <td>0.849582</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.635417</td>\n",
       "      <td>0.491965</td>\n",
       "      <td>0.503827</td>\n",
       "      <td>0.596070</td>\n",
       "      <td>0.557604</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>0.013122</td>\n",
       "      <td>0.013631</td>\n",
       "      <td>0.022183</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>0.111327</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.004858</td>\n",
       "      <td>-0.857056</td>\n",
       "      <td>-0.084885</td>\n",
       "      <td>0.474686</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.563401</td>\n",
       "      <td>0.944010</td>\n",
       "      <td>1.878005</td>\n",
       "      <td>-0.864478</td>\n",
       "      <td>-0.095041</td>\n",
       "      <td>0.466776</td>\n",
       "      <td>9.765625e-08</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>0.069561</td>\n",
       "      <td>-0.096004</td>\n",
       "      <td>0.077474</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.009589</td>\n",
       "      <td>0.009925</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.402547</td>\n",
       "      <td>1.013362</td>\n",
       "      <td>1.039248</td>\n",
       "      <td>0.99202</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>-0.233078</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>1.437717</td>\n",
       "      <td>1.255580</td>\n",
       "      <td>1.489258</td>\n",
       "      <td>0.974197</td>\n",
       "      <td>-0.014669</td>\n",
       "      <td>-0.249680</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.015346</td>\n",
       "      <td>-0.161628</td>\n",
       "      <td>0.199220</td>\n",
       "      <td>0.013709</td>\n",
       "      <td>0.010763</td>\n",
       "      <td>0.014762</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.806361</td>\n",
       "      <td>1.094488</td>\n",
       "      <td>1.251700</td>\n",
       "      <td>0.176189</td>\n",
       "      <td>0.485587</td>\n",
       "      <td>0.159029</td>\n",
       "      <td>-0.841541</td>\n",
       "      <td>0.133141</td>\n",
       "      <td>0.061469</td>\n",
       "      <td>0.043901</td>\n",
       "      <td>4.332320</td>\n",
       "      <td>2.887801</td>\n",
       "      <td>12.346117</td>\n",
       "      <td>165.242446</td>\n",
       "      <td>140.565259</td>\n",
       "      <td>615.469160</td>\n",
       "      <td>0.175885</td>\n",
       "      <td>0.485399</td>\n",
       "      <td>0.158387</td>\n",
       "      <td>-0.841644</td>\n",
       "      <td>0.050176</td>\n",
       "      <td>0.020933</td>\n",
       "      <td>-0.022867</td>\n",
       "      <td>0.428492</td>\n",
       "      <td>0.224097</td>\n",
       "      <td>3.416566</td>\n",
       "      <td>-216.183273</td>\n",
       "      <td>-80.686083</td>\n",
       "      <td>232.653044</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.063817</td>\n",
       "      <td>0.036748</td>\n",
       "      <td>0.077514</td>\n",
       "      <td>3.233995</td>\n",
       "      <td>1.553890</td>\n",
       "      <td>5.736241</td>\n",
       "      <td>586.244076</td>\n",
       "      <td>234.136553</td>\n",
       "      <td>556.915683</td>\n",
       "      <td>0.603038</td>\n",
       "      <td>-0.175326</td>\n",
       "      <td>-0.766516</td>\n",
       "      <td>-0.128901</td>\n",
       "      <td>0.115731</td>\n",
       "      <td>0.321439</td>\n",
       "      <td>0.121293</td>\n",
       "      <td>5.012583</td>\n",
       "      <td>12.815405</td>\n",
       "      <td>7.555850</td>\n",
       "      <td>224.865107</td>\n",
       "      <td>706.861321</td>\n",
       "      <td>549.917517</td>\n",
       "      <td>0.600378</td>\n",
       "      <td>-0.176324</td>\n",
       "      <td>-0.769133</td>\n",
       "      <td>-0.129948</td>\n",
       "      <td>0.030569</td>\n",
       "      <td>0.132933</td>\n",
       "      <td>-0.023579</td>\n",
       "      <td>1.005235</td>\n",
       "      <td>2.467654</td>\n",
       "      <td>-1.775671</td>\n",
       "      <td>2.89280</td>\n",
       "      <td>-87.699311</td>\n",
       "      <td>-108.119283</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.055740</td>\n",
       "      <td>0.140247</td>\n",
       "      <td>0.158323</td>\n",
       "      <td>2.578049</td>\n",
       "      <td>8.649329</td>\n",
       "      <td>8.466336</td>\n",
       "      <td>187.474756</td>\n",
       "      <td>696.353488</td>\n",
       "      <td>622.765618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant_id  clothes_id  property_id property_name  interaction_id  \\\n",
       "0               7          14            0    smoothness               1   \n",
       "1               7          14            0    smoothness               1   \n",
       "\n",
       "   rating rating_level  rating_level_num  sub_window_num  slice_num  \\\n",
       "0       3       medium                 2               1          1   \n",
       "1       3       medium                 2               1          2   \n",
       "\n",
       "   max_ch1_hand0  max_ch2_hand0  max_ch3_hand0  max_ch4_hand0  max_ch5_hand0  \\\n",
       "0            0.5       0.615385       0.670391       0.495868       0.759036   \n",
       "1            0.5       0.615385       0.670391       0.495868       0.734940   \n",
       "\n",
       "   max_ch6_hand0  max_ch7_hand0  max_ch8_hand0  mean_ch1_hand0  \\\n",
       "0       0.652406        0.75625       0.666667        0.497805   \n",
       "1       0.641711        0.75625       0.666667        0.497331   \n",
       "\n",
       "   mean_ch2_hand0  mean_ch3_hand0  mean_ch4_hand0  mean_ch5_hand0  \\\n",
       "0        0.613462        0.665764        0.492188        0.718656   \n",
       "1        0.612607        0.665735        0.492597        0.717746   \n",
       "\n",
       "   mean_ch6_hand0  mean_ch7_hand0  mean_ch8_hand0  std_ch1_hand0  \\\n",
       "0        0.636197        0.748926        0.660851       0.002324   \n",
       "1        0.635807        0.749609        0.661458       0.002484   \n",
       "\n",
       "   std_ch2_hand0  std_ch3_hand0  std_ch4_hand0  std_ch5_hand0  std_ch6_hand0  \\\n",
       "0       0.002503       0.003077       0.002455       0.006501       0.004039   \n",
       "1       0.002791       0.003327       0.002405       0.008698       0.005318   \n",
       "\n",
       "   std_ch7_hand0  std_ch8_hand0  max_ch1_hand1  max_ch2_hand1  max_ch3_hand1  \\\n",
       "0       0.003061       0.002083       0.508197       0.908451       0.751445   \n",
       "1       0.003954       0.002901       0.504098       0.929578       0.757225   \n",
       "\n",
       "   max_ch4_hand1  max_ch5_hand1  max_ch6_hand1  max_ch7_hand1  max_ch8_hand1  \\\n",
       "0       0.730159       0.493878       0.995918       0.648515       0.576037   \n",
       "1       0.714286       0.497959       0.734694       0.633663       0.571429   \n",
       "\n",
       "   mean_ch1_hand1  mean_ch2_hand1  mean_ch3_hand1  mean_ch4_hand1  \\\n",
       "0        0.496670        0.846831        0.691595        0.636244   \n",
       "1        0.497375        0.849582        0.692467        0.635417   \n",
       "\n",
       "   mean_ch5_hand1  mean_ch6_hand1  mean_ch7_hand1  mean_ch8_hand1  \\\n",
       "0        0.491412        0.608674        0.601382        0.558852   \n",
       "1        0.491965        0.503827        0.596070        0.557604   \n",
       "\n",
       "   std_ch1_hand1  std_ch2_hand1  std_ch3_hand1  std_ch4_hand1  std_ch5_hand1  \\\n",
       "0       0.004198       0.014491       0.015432       0.026432       0.002186   \n",
       "1       0.002954       0.013122       0.013631       0.022183       0.002619   \n",
       "\n",
       "   std_ch6_hand1  std_ch7_hand1  std_ch8_hand1  max_Ax_hand0  max_Ay_hand0  \\\n",
       "0       0.218816       0.022186       0.006429     -0.857056     -0.084885   \n",
       "1       0.111327       0.012712       0.004858     -0.857056     -0.084885   \n",
       "\n",
       "   max_Az_hand0  max_Vx_hand0  max_Vy_hand0  max_Vz_hand0  max_Jx_hand0  \\\n",
       "0      0.479568      0.000208      0.000215      0.000349      0.718061   \n",
       "1      0.474686      0.000095      0.000256      0.000317      0.563401   \n",
       "\n",
       "   max_Jy_hand0  max_Jz_hand0  mean_Ax_hand0  mean_Ay_hand0  mean_Az_hand0  \\\n",
       "0      0.488281      1.206342      -0.863110      -0.091721       0.468826   \n",
       "1      0.944010      1.878005      -0.864478      -0.095041       0.466776   \n",
       "\n",
       "   mean_Vx_hand0  mean_Vy_hand0  mean_Vz_hand0  mean_Jx_hand0  mean_Jy_hand0  \\\n",
       "0   9.082031e-06      -0.000028      -0.000006       0.036854       0.039343   \n",
       "1   9.765625e-08      -0.000016      -0.000067       0.069561      -0.096004   \n",
       "\n",
       "   mean_Jz_hand0  std_Ax_hand0  std_Ay_hand0  std_Az_hand0  std_Vx_hand0  \\\n",
       "0       0.092439      0.006753      0.007619      0.010341      0.000193   \n",
       "1       0.077474      0.004900      0.009589      0.009925      0.000092   \n",
       "\n",
       "   std_Vy_hand0  std_Vz_hand0  std_Jx_hand0  std_Jy_hand0  std_Jz_hand0  \\\n",
       "0      0.000245      0.000281      0.488563      0.441082      0.750762   \n",
       "1      0.000279      0.000253      0.402547      1.013362      1.039248   \n",
       "\n",
       "   max_Ax_hand1  max_Ay_hand1  max_Az_hand1  max_Vx_hand1  max_Vy_hand1  \\\n",
       "0       0.98616      0.013489     -0.211105      0.001218      0.000510   \n",
       "1       0.99202      0.002258     -0.233078      0.000466      0.000281   \n",
       "\n",
       "   max_Vz_hand1  max_Jx_hand1  max_Jy_hand1  max_Jz_hand1  mean_Ax_hand1  \\\n",
       "0      0.000937      1.448006      0.708008      2.343750       0.964774   \n",
       "1      0.000830      1.437717      1.255580      1.489258       0.974197   \n",
       "\n",
       "   mean_Ay_hand1  mean_Az_hand1  mean_Vx_hand1  mean_Vy_hand1  mean_Vz_hand1  \\\n",
       "0      -0.007117      -0.249875       0.000112       0.000070       0.000027   \n",
       "1      -0.014669      -0.249680      -0.000012      -0.000108       0.000176   \n",
       "\n",
       "   mean_Jx_hand1  mean_Jy_hand1  mean_Jz_hand1  std_Ax_hand1  std_Ay_hand1  \\\n",
       "0      -0.104780       0.096063       0.335256      0.022364      0.013963   \n",
       "1       0.015346      -0.161628       0.199220      0.013709      0.010763   \n",
       "\n",
       "   std_Az_hand1  std_Vx_hand1  std_Vy_hand1  std_Vz_hand1  std_Jx_hand1  \\\n",
       "0      0.028878      0.000738      0.000331      0.000943      1.260192   \n",
       "1      0.014762      0.000277      0.000409      0.000450      0.806361   \n",
       "\n",
       "   std_Jy_hand1  std_Jz_hand1  max_w_hand0  max_x_hand0  max_y_hand0  \\\n",
       "0      0.571902      1.565671     0.179352     0.484946     0.160615   \n",
       "1      1.094488      1.251700     0.176189     0.485587     0.159029   \n",
       "\n",
       "   max_z_hand0  max_AVx_hand0  max_AVy_hand0  max_AVz_hand0  max_AAx_hand0  \\\n",
       "0    -0.840857       0.009831       0.012959       0.117029       1.270143   \n",
       "1    -0.841541       0.133141       0.061469       0.043901       4.332320   \n",
       "\n",
       "   max_AAy_hand0  max_AAz_hand0  max_AJx_hand0  max_AJy_hand0  max_AJz_hand0  \\\n",
       "0       2.238883      19.812834     154.113175     104.818227    1338.775861   \n",
       "1       2.887801      12.346117     165.242446     140.565259     615.469160   \n",
       "\n",
       "   mean_w_hand0  mean_x_hand0  mean_y_hand0  mean_z_hand0  mean_AVx_hand0  \\\n",
       "0      0.178765      0.484513      0.160469     -0.841154       -0.016003   \n",
       "1      0.175885      0.485399      0.158387     -0.841644        0.050176   \n",
       "\n",
       "   mean_AVy_hand0  mean_AVz_hand0  mean_AAx_hand0  mean_AAy_hand0  \\\n",
       "0       -0.027618       -0.081836        0.129868        0.552890   \n",
       "1        0.020933       -0.022867        0.428492        0.224097   \n",
       "\n",
       "   mean_AAz_hand0  mean_AJx_hand0  mean_AJy_hand0  mean_AJz_hand0  \\\n",
       "0       -1.016133       16.779031      -45.915024      210.907738   \n",
       "1        3.416566     -216.183273      -80.686083      232.653044   \n",
       "\n",
       "   std_w_hand0  std_x_hand0  std_y_hand0  std_z_hand0  std_AVx_hand0  \\\n",
       "0     0.000451     0.000472     0.000210     0.000335       0.016423   \n",
       "1     0.000331     0.000123     0.000423     0.000076       0.063817   \n",
       "\n",
       "   std_AVy_hand0  std_AVz_hand0  std_AAx_hand0  std_AAy_hand0  std_AAz_hand0  \\\n",
       "0       0.031903       0.200440       1.160057       1.546528      13.337295   \n",
       "1       0.036748       0.077514       3.233995       1.553890       5.736241   \n",
       "\n",
       "   std_AJx_hand0  std_AJy_hand0  std_AJz_hand0  max_w_hand1  max_x_hand1  \\\n",
       "0      97.220032     155.178181    1114.885458     0.600343    -0.175433   \n",
       "1     586.244076     234.136553     556.915683     0.603038    -0.175326   \n",
       "\n",
       "   max_y_hand1  max_z_hand1  max_AVx_hand1  max_AVy_hand1  max_AVz_hand1  \\\n",
       "0    -0.769104    -0.129347       0.140072       0.440644       0.126734   \n",
       "1    -0.766516    -0.128901       0.115731       0.321439       0.121293   \n",
       "\n",
       "   max_AAx_hand1  max_AAy_hand1  max_AAz_hand1  max_AJx_hand1  max_AJy_hand1  \\\n",
       "0       5.745156      20.612016       5.331893     212.817696     779.034203   \n",
       "1       5.012583      12.815405       7.555850     224.865107     706.861321   \n",
       "\n",
       "   max_AJz_hand1  mean_w_hand1  mean_x_hand1  mean_y_hand1  mean_z_hand1  \\\n",
       "0     579.105295      0.598260     -0.176827     -0.770695     -0.129768   \n",
       "1     549.917517      0.600378     -0.176324     -0.769133     -0.129948   \n",
       "\n",
       "   mean_AVx_hand1  mean_AVy_hand1  mean_AVz_hand1  mean_AAx_hand1  \\\n",
       "0        0.053843        0.125821       -0.043329        0.487977   \n",
       "1        0.030569        0.132933       -0.023579        1.005235   \n",
       "\n",
       "   mean_AAy_hand1  mean_AAz_hand1  mean_AJx_hand1  mean_AJy_hand1  \\\n",
       "0        0.515185       -1.737315       -44.05619      -91.541428   \n",
       "1        2.467654       -1.775671         2.89280      -87.699311   \n",
       "\n",
       "   mean_AJz_hand1  std_w_hand1  std_x_hand1  std_y_hand1  std_z_hand1  \\\n",
       "0       81.034379     0.002302     0.000906     0.001676     0.000510   \n",
       "1     -108.119283     0.001406     0.000820     0.001312     0.000867   \n",
       "\n",
       "   std_AVx_hand1  std_AVy_hand1  std_AVz_hand1  std_AAx_hand1  std_AAy_hand1  \\\n",
       "0       0.073437       0.280630       0.115024       3.958058      13.903979   \n",
       "1       0.055740       0.140247       0.158323       2.578049       8.649329   \n",
       "\n",
       "   std_AAz_hand1  std_AJx_hand1  std_AJy_hand1  std_AJz_hand1  \n",
       "0       6.202001     170.597999     792.769940     449.786951  \n",
       "1       8.466336     187.474756     696.353488     622.765618  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_data = physical_df.iloc[:36000,:]\n",
    "new_data = physical_df.iloc[36000:,:]\n",
    "\n",
    "existing_data.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000, 180)\n",
      "(26989, 180)\n",
      "(36000, 180)\n",
      "(26989, 180)\n"
     ]
    }
   ],
   "source": [
    "starting_index = 10\n",
    "\n",
    "# Create a df of features\n",
    "existing_features_df = existing_data.iloc[:,starting_index:]\n",
    "new_features_df = new_data.iloc[:,starting_index:]\n",
    "\n",
    "# Create a df with the first 10 columns\n",
    "existing_info = existing_data.iloc[:, :starting_index]\n",
    "new_info = new_data.iloc[:, :starting_index]\n",
    "\n",
    "normalised_existing_features = existing_features_df.copy()\n",
    "normalised_new_features = new_features_df.copy()\n",
    "print(normalised_existing_features.shape)\n",
    "print(normalised_new_features.shape)\n",
    "\n",
    "# create scaler\n",
    "scaler = MinMaxScaler(feature_range=(-1,1)) # As this is the range of the activation function - tanh\n",
    "\n",
    "# fit scaler and apply transform\n",
    "normalised_existing_features[normalised_existing_features.columns] = scaler.fit_transform(existing_features_df[existing_features_df.columns])\n",
    "normalised_new_features[normalised_new_features.columns] = scaler.fit_transform(new_features_df[new_features_df.columns])\n",
    "\n",
    "print(normalised_existing_features.shape)\n",
    "print(normalised_new_features.shape)\n",
    "\n",
    "normalised_existing_df = pd.concat([existing_info, normalised_existing_features], axis=1)\n",
    "normalised_new_df = pd.concat([new_info, normalised_new_features], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create X and y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_2d(df, features_starting_idx):\n",
    "    \n",
    "    X_2d = df.iloc[:,features_starting_idx:].values\n",
    "    \n",
    "    X_tensor_2d = torch.Tensor(X_2d)    \n",
    "    return X_tensor_2d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y_train_for_2d_X(df, predicting_feature = 'property_id', output_as_tensor='Yes'):\n",
    "    # CreatE an instance of a one-hot-encoder\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Perform one-hot encoding on the specified column \n",
    "    encoder_df = pd.DataFrame(encoder.fit_transform(df[[predicting_feature]]).toarray())\n",
    "    \n",
    "    # Convert to a numpy array\n",
    "    y_train = encoder_df.to_numpy()\n",
    "    \n",
    "    if output_as_tensor == 'Yes':\n",
    "        # Convert to a tensor\n",
    "        y_train = torch.Tensor(y_train)\n",
    "\n",
    "    return y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y_test_for_2d_X(df, predicting_feature = 'property_id'):   \n",
    "    y_test = df[predicting_feature].values\n",
    "    #if predicting_feature == 'property_id':\n",
    "       # y_test = y_test - 3\n",
    "    if predicting_feature == 'rating_level_num':\n",
    "        y_test = y_test - 1\n",
    "    \n",
    "    y_test_tensor = torch.Tensor(y_test)    \n",
    "    y_test_tensor = y_test_tensor.type(torch.LongTensor)\n",
    "    \n",
    "    return y_test_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_3d(df, features_starting_idx):\n",
    "    dim1 = df.new_interaction_id.nunique()\n",
    "    print(dim1)\n",
    "    dim2 = df.slice_num.nunique()\n",
    "    dim3 = df.iloc[:,features_starting_idx:].shape[1]\n",
    "        \n",
    "    X = np.zeros((dim1, dim2, dim3)) \n",
    "\n",
    "    itr_id_lst = df.new_interaction_id.unique().tolist()\n",
    "    print(itr_id_lst[0], itr_id_lst[-1])\n",
    "\n",
    "    for itr_id in itr_id_lst: #range(len(itr_id_lst)):\n",
    "        #itr_id = itr_id_lst[i]\n",
    "        itr_id_df = df[df.new_interaction_id==itr_id]  \n",
    "        \n",
    "        for j in range(itr_id_df.shape[0]):\n",
    "            vals_arr = itr_id_df.iloc[j,features_starting_idx:].values\n",
    "            if itr_id-1 == dim1:\n",
    "                print(itr_id)\n",
    "            X[itr_id-1,j] = vals_arr\n",
    "    \n",
    "    X_tensor = torch.Tensor(X)    \n",
    "    return X_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y_train_for_3d_X(df, predicting_feature = 'property_id'):\n",
    "    # Create a dataset with only the required columns\n",
    "    df2 = df[['new_interaction_id', 'property_id', 'rating_level_num']]\n",
    "\n",
    "    # Remove duplicates\n",
    "    df2.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "    # Reset the indexes\n",
    "    df2.reset_index(drop=True, inplace=True) \n",
    "    \n",
    "    ## Create y train\n",
    "    # CreatE an instance of a one-hot-encoder\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Perform one-hot encoding on the specified column \n",
    "    encoder_df = pd.DataFrame(encoder.fit_transform(df2[[predicting_feature]]).toarray())\n",
    "    \n",
    "    # Convert to a numpy array\n",
    "    y_train = encoder_df.to_numpy()\n",
    "    \n",
    "    # Convert to a tensor\n",
    "    y_train = torch.Tensor(y_train)\n",
    "  \n",
    "    return y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y_test_for_3d_X(df, predicting_feature = 'property_id'):\n",
    "    # Create a dataset with only the required columns\n",
    "    df2 = df[['new_interaction_id', 'property_id', 'rating_level_num']]\n",
    "\n",
    "    # Remove duplicates\n",
    "    df2.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "    # Reset the indexes\n",
    "    df2.reset_index(drop=True, inplace=True) \n",
    "    \n",
    "    y_test = df2[predicting_feature].values\n",
    "    #if predicting_feature == 'property_id':\n",
    "       # y_test = y_test - 3\n",
    "    if predicting_feature == 'rating_level_num':\n",
    "        y_test = y_test - 1\n",
    "    \n",
    "    y_test = torch.Tensor(y_test)    \n",
    "    y_test = y_test.type(torch.LongTensor)\n",
    "    \n",
    "    return y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model 1 - Linear model using all 180 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_all_features_properties(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(180, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 5)        \n",
    "           \n",
    "    def forward(self, x): #, x2\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "class Linear_all_features_ratings(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(185, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)        \n",
    "\n",
    "    def forward(self, x): #, x2\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_all_features_properties(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(90, 40, 1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(10 * 40 * 2, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 5)\n",
    "          \n",
    "    def forward(self, x1, x2): #, x2\n",
    "        #print(x1.shape)\n",
    "        x1, (hn, cn) = self.rnn(x1) #, (self.h0, self.c0)\n",
    "        x1 = F.tanh(x1)\n",
    "        #print(x2.shape)\n",
    "        x2, (hm, cm) = self.rnn(x2) # (self.h0, self.c0)\n",
    "        x2 = F.tanh(x2)\n",
    "        #print(x1.shape,x2.shape)\n",
    "        x = torch.cat((x1, x2), 2)\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model_for_2d_X(train_dataloader, learning_rate, num_epochs, model):\n",
    "\n",
    "    # Model\n",
    "    train_model = model\n",
    "\n",
    "    # Loss and Optimiser\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(train_model.parameters(), lr=learning_rate, momentum=0.7)\n",
    "\n",
    "    best_train_loss = np.inf\n",
    "    best_model = None\n",
    "    #best_model_epoch_num = np.inf\n",
    "    train_loss_lst = []\n",
    "    #val_loss_lst = []\n",
    "    #avg_loss_lst = []\n",
    "\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        #Set the model in training mode\n",
    "        train_model.train()\n",
    "\n",
    "        # Initialise the total training and validation loss\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "        avg_loss = 0\n",
    "\n",
    "        for i, train_data in enumerate(train_dataloader, 0):\n",
    "\n",
    "            # get the inputs; data is a list of [input1, input2, label]\n",
    "            train_input1, train_labels = train_data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            train_preds = train_model(train_input1) #, train_input2\n",
    "            train_loss = criterion(train_preds, train_labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update training loss\n",
    "            epoch_train_loss += train_loss.item()\n",
    "      \n",
    "        avg_training_loss = epoch_train_loss / len(train_dataloader) \n",
    "        \n",
    "        train_loss_lst.append(avg_training_loss)\n",
    "         \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'epoch {epoch+1}: train loss = {round(avg_training_loss,2)}')\n",
    "\n",
    "        if avg_training_loss < best_train_loss:\n",
    "            best_train_loss = avg_training_loss\n",
    "            best_model = train_model.state_dict()\n",
    "\n",
    "    return best_train_loss, best_model, train_loss_lst   #, avg_loss_lst, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_best_model_for_3d_X(train_dataloader, learning_rate, num_epochs, model):\n",
    "\n",
    "    # Model\n",
    "    train_model = model\n",
    "\n",
    "    # Loss and Optimiser\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(train_model.parameters(), lr=learning_rate, momentum=0.7)\n",
    "\n",
    "    best_avg_loss = np.inf\n",
    "    best_model = None\n",
    "    #best_model_epoch_num = np.inf\n",
    "    train_loss_lst = []\n",
    "   # val_loss_lst = []\n",
    "   # avg_loss_lst = []\n",
    "\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        #Set the model in training mode\n",
    "        train_model.train()\n",
    "\n",
    "        # Initialise the total training and validation loss\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "        avg_loss = 0\n",
    "\n",
    "        #running_loss = 0.0\n",
    "        for i, train_data in enumerate(train_dataloader, 0):\n",
    "            #print(len(train_data))\n",
    "\n",
    "            # get the inputs; data is a list of [input1, input2, label]\n",
    "            train_input1, train_input2, train_labels = train_data #train_input2, \n",
    "\n",
    "            #train_labels = train_labels.type(torch.LongTensor)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            train_preds = train_model(train_input1, train_input2)  \n",
    "            #print(train_labels)#\n",
    "\n",
    "            train_loss = criterion(train_preds, train_labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update training loss\n",
    "            epoch_train_loss += train_loss.item()\n",
    "\n",
    "     \n",
    "        avg_training_loss = epoch_train_loss / len(train_dataloader) #count_train\n",
    "        \n",
    "        train_loss_lst.append(avg_training_loss)\n",
    "         \n",
    "        #print(f'epoch {epoch+1}: train loss = {round(avg_training_loss,3)}, val loss = {round(avg_validation_loss,3)}, average loss = {round(avg_loss,3)}')\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'epoch {epoch+1}: train loss = {round(avg_training_loss,2)}')\n",
    "        \n",
    "        if avg_training_loss < best_train_loss:\n",
    "            best_train_loss = avg_training_loss\n",
    "            best_model = train_model.state_dict()\n",
    "\n",
    "    return best_train_loss, best_model, train_loss_lst \n",
    "\n",
    "    #return best_avg_loss, best_model, train_loss_lst, val_loss_lst, avg_loss_lst   #, avg_loss_lst, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "def Linear_LOP0CV(lili_data, dat_2022, model, num_folds=5, predicting_feature='property_id', learning_rate=0.01, num_epochs=10, random_state=num): #, num_inner_folds=5\n",
    "    # Set fixed random number seed\n",
    "    torch.manual_seed(num)\n",
    "    \n",
    "    total_conf_mat = 0\n",
    "    micro_f1_lst = []\n",
    "    acc_lst = []\n",
    "    if predicting_feature == 'property_id':\n",
    "        macro_f1_lst = []\n",
    "    elif predicting_feature == 'rating_level_num':\n",
    "        weighted_f1_lst = []  \n",
    "        \n",
    "    # Data preparation\n",
    "    X_train = create_X_2d(lili_data, 10)       \n",
    "    y_train = create_y_train_for_2d_X(lili_data, predicting_feature = predicting_feature, output_as_tensor='Yes')\n",
    "    X_test = create_X_2d(dat_2022, 10) \n",
    "    y_test = create_y_test_for_2d_X(dat_2022, predicting_feature = predicting_feature)\n",
    "        \n",
    "    if predicting_feature == 'rating_level_num':\n",
    "        X_train_add = create_y_train_for_2d_X(lili_data, predicting_feature = 'property_id', output_as_tensor='Yes')\n",
    "        X_train = torch.cat((X_train, X_train_add), 1)\n",
    "        X_test_add = create_y_train_for_2d_X(dat_2022, predicting_feature = 'property_id', output_as_tensor='Yes')  \n",
    "        X_test = torch.cat((X_test, X_test_add), 1)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=y_train.shape[0]) # num_workers=2,\n",
    "    \n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, shuffle=True, batch_size=y_test.shape[0]) # num_workers=2,       \n",
    "\n",
    "    avg_training_loss, best_model, training_loss_lst = find_best_model_for_2d_X(train_dataloader, learning_rate, num_epochs, model)\n",
    "\n",
    "    # save trained model \n",
    "    name = 'best_model.pt'\n",
    "    torch.save(best_model, name)\n",
    "    print(f'The model has been saved')\n",
    "\n",
    "    test_model = model\n",
    "    test_model.load_state_dict(torch.load(name))\n",
    "\n",
    "    dataiter = iter(test_dataloader) \n",
    "    test_input1, test_labels = dataiter.next() \n",
    "\n",
    "    test_preds = test_model(test_input1) \n",
    "\n",
    "    test_preds_np = test_preds.detach().numpy()\n",
    "    test_predicted_np = np.argmax(test_preds_np, axis = 1)\n",
    "\n",
    "    test_labels_np = test_labels.numpy()       \n",
    "\n",
    "    if predicting_feature == 'property_id':\n",
    "        conf_mat = confusion_matrix(test_labels_np, test_predicted_np, labels=[0, 1, 2, 3, 4])\n",
    "        macro_f1_score = f1_score(test_labels_np, test_predicted_np, average='macro') \n",
    "        macro_f1_lst.append(macro_f1_score) \n",
    "    elif predicting_feature == 'rating_level_num':\n",
    "        conf_mat = confusion_matrix(test_labels_np, test_predicted_np, labels=[0,1,2])\n",
    "        weighted_f1_score = f1_score(test_labels_np, test_predicted_np, average='weighted') \n",
    "        weighted_f1_lst.append(weighted_f1_score)\n",
    "\n",
    "    total_conf_mat += conf_mat\n",
    "    micro_f1_score = f1_score(test_labels_np, test_predicted_np, average='micro')  \n",
    "    micro_f1_lst.append(micro_f1_score)\n",
    "    acc = accuracy_score(test_labels_np, test_predicted_np)\n",
    "    acc_lst.append(acc)\n",
    "\n",
    "\n",
    "    #print(f\"Leaving participant {participant} out\")\n",
    "    print(\"(1) Confusion matrix:\\n\", conf_mat)\n",
    "    print(f\"(2) micro F1 score = {round(micro_f1_score,2)}\") \n",
    "    if predicting_feature == 'property_id':\n",
    "        print(f\"(3) Macro F1 score = {round(macro_f1_score,2)}\")\n",
    "    elif predicting_feature == 'rating_level_num':\n",
    "        print(f\"(3) Weighted F1 score = {round(weighted_f1_score,2)}\")            \n",
    "    print(f\"(4) Percentage Classification accuracy = {round(acc*100,2)}%\")\n",
    "\n",
    "    print('--------------------------------')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "def LSTM_LOP0CV(lili_0, lili_1, new_0, new_1, model, num_folds=5, predicting_feature='property_id', learning_rate=0.01, num_epochs=10, random_state=num): #, num_inner_folds=5\n",
    "    # Set fixed random number seed\n",
    "    torch.manual_seed(num)\n",
    "        \n",
    "    total_conf_mat = 0\n",
    "    micro_f1_lst = []\n",
    "    acc_lst = []\n",
    "    if predicting_feature == 'property_id':\n",
    "        macro_f1_lst = []\n",
    "    elif predicting_feature == 'rating_level_num':\n",
    "        weighted_f1_lst = [] \n",
    "        \n",
    "        data0_add = create_y_train_for_2d_X(data_0, predicting_feature = 'property_id', output_as_tensor='No')\n",
    "        data0_add_pd = pd.DataFrame(data0_add, columns = ['smoothness','thickness','warmth', 'flexibility', 'softness'])\n",
    "        data_0 = pd.concat([data_0.reset_index(drop=True), data0_add_pd.reset_index(drop=True)], axis=1)\n",
    "        data1_add = create_y_train_for_2d_X(data_1, predicting_feature = 'property_id', output_as_tensor='No')\n",
    "        data1_add_pd = pd.DataFrame(data1_add, columns = ['smoothness','thickness','warmth', 'flexibility', 'softness'])\n",
    "        data_1 = pd.concat([data_1.reset_index(drop=True), data1_add_pd.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    #for participant in sorted(data_0.participant_id.unique()): # # #:lst: #\n",
    "       # print(f'LEAVING PARTICIPANT {participant} OUT:')\n",
    "        \n",
    "        # Split the data into training and testing\n",
    "        #training_data_0 = data_0[data_0.participant_id != participant] \n",
    "        #training_data_1 = data_1[data_1.participant_id != participant] \n",
    "        #testing_data_0 = data_0[data_0.participant_id == participant] \n",
    "        #testing_data_1 = data_1[data_1.participant_id == participant] \n",
    "\n",
    "    # Data preparation\n",
    "    X_train_0 = create_X_3d(lili_0, 10)\n",
    "    X_train_1 = create_X_3d(lili_1, 10) \n",
    "    X_test_0 = create_X_3d(new_0, 10) \n",
    "    X_test_1 = create_X_3d(new_1, 10)           \n",
    "    y_train = create_y_train_for_3d_X(lili_0, predicting_feature = predicting_feature)\n",
    "    y_test = create_y_test_for_3d_X(new_0, predicting_feature = predicting_feature)        \n",
    "\n",
    "\n",
    "        #print(X_train_0.shape)\n",
    "        #print(X_train_1.shape)\n",
    "        #print(y_train.shape)\n",
    "        #print(X_test_0.shape)\n",
    "        #print(X_test_1.shape)\n",
    "        #print(y_test.shape)\n",
    " \n",
    "    # Create the datasets and dataloaders\n",
    "    train_dataset = TensorDataset(X_train_0, X_train_1, y_train) \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=y_train.shape[0]) # num_workers=2,\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_0, X_test_1, y_test)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, shuffle=True, batch_size=y_test.shape[0]) # num_workers=2,       \n",
    "            \n",
    "    train_loss, best_model, train_loss_lst = find_best_model_for_3d_X(train_dataloader, learning_rate, num_epochs, model)\n",
    "                            \n",
    "    # save trained model \n",
    "    name = 'best_model.pt'\n",
    "    torch.save(best_model, name)\n",
    "    print(f'The model has been saved')\n",
    "      \n",
    "        \n",
    "    test_model = model\n",
    "    test_model.load_state_dict(torch.load(name))\n",
    "\n",
    "    dataiter = iter(test_dataloader) \n",
    "    test_input1, test_input2, test_labels = dataiter.next() \n",
    "\n",
    "    test_preds = test_model(test_input1, test_input2) \n",
    "\n",
    "    test_preds_np = test_preds.detach().numpy()\n",
    "    test_predicted_np = np.argmax(test_preds_np, axis = 1)\n",
    "\n",
    "    test_labels_np = test_labels.numpy()    \n",
    "    \n",
    "    if predicting_feature == 'property_id':\n",
    "        conf_mat = confusion_matrix(test_labels_np, test_predicted_np, labels=[0, 1, 2, 3, 4])\n",
    "        macro_f1_score = f1_score(test_labels_np, test_predicted_np, average='macro') \n",
    "        #macro_f1_lst.append(macro_f1_score) \n",
    "    elif predicting_feature == 'rating_level_num':\n",
    "        conf_mat = confusion_matrix(test_labels_np, test_predicted_np, labels=[0,1,2])\n",
    "        weighted_f1_score = f1_score(test_labels_np, test_predicted_np, average='weighted') \n",
    "        weighted_f1_lst.append(weighted_f1_score)\n",
    "\n",
    "    #total_conf_mat += conf_mat\n",
    "    micro_f1_score = f1_score(test_labels_np, test_predicted_np, average='micro')  \n",
    "    #micro_f1_lst.append(micro_f1_score)\n",
    "    acc = accuracy_score(test_labels_np, test_predicted_np)\n",
    "    #acc_lst.append(acc)\n",
    "\n",
    "\n",
    "    #print(f\"Leaving participant {participant} out\")\n",
    "    print(\"(1) Confusion matrix:\\n\", conf_mat)\n",
    "    print(f\"(2) micro F1 score = {round(micro_f1_score,2)}\") \n",
    "    if predicting_feature == 'property_id':\n",
    "        print(f\"(3) Macro F1 score = {round(macro_f1_score,2)}\")\n",
    "    elif predicting_feature == 'rating_level_num':\n",
    "        print(f\"(3) Weighted F1 score = {round(weighted_f1_score,2)}\")            \n",
    "    print(f\"(4) Percentage Classification accuracy = {round(acc*100,2)}%\")\n",
    "\n",
    "    print('--------------------------------')\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run the FC for properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss = 1.61\n",
      "epoch 11: train loss = 1.61\n",
      "epoch 21: train loss = 1.61\n",
      "epoch 31: train loss = 1.61\n",
      "epoch 41: train loss = 1.61\n",
      "epoch 51: train loss = 1.61\n",
      "epoch 61: train loss = 1.61\n",
      "epoch 71: train loss = 1.61\n",
      "epoch 81: train loss = 1.61\n",
      "epoch 91: train loss = 1.61\n",
      "epoch 101: train loss = 1.61\n",
      "epoch 111: train loss = 1.61\n",
      "epoch 121: train loss = 1.61\n",
      "epoch 131: train loss = 1.61\n",
      "epoch 141: train loss = 1.61\n",
      "epoch 151: train loss = 1.61\n",
      "epoch 161: train loss = 1.61\n",
      "epoch 171: train loss = 1.61\n",
      "epoch 181: train loss = 1.61\n",
      "epoch 191: train loss = 1.61\n",
      "epoch 201: train loss = 1.61\n",
      "epoch 211: train loss = 1.61\n",
      "epoch 221: train loss = 1.61\n",
      "epoch 231: train loss = 1.61\n",
      "epoch 241: train loss = 1.61\n",
      "epoch 251: train loss = 1.61\n",
      "epoch 261: train loss = 1.61\n",
      "epoch 271: train loss = 1.61\n",
      "epoch 281: train loss = 1.61\n",
      "epoch 291: train loss = 1.61\n",
      "epoch 301: train loss = 1.61\n",
      "epoch 311: train loss = 1.61\n",
      "epoch 321: train loss = 1.61\n",
      "epoch 331: train loss = 1.61\n",
      "epoch 341: train loss = 1.61\n",
      "epoch 351: train loss = 1.61\n",
      "epoch 361: train loss = 1.61\n",
      "epoch 371: train loss = 1.61\n",
      "epoch 381: train loss = 1.61\n",
      "epoch 391: train loss = 1.61\n",
      "epoch 401: train loss = 1.61\n",
      "epoch 411: train loss = 1.61\n",
      "epoch 421: train loss = 1.61\n",
      "epoch 431: train loss = 1.61\n",
      "epoch 441: train loss = 1.61\n",
      "epoch 451: train loss = 1.61\n",
      "epoch 461: train loss = 1.61\n",
      "epoch 471: train loss = 1.61\n",
      "epoch 481: train loss = 1.61\n",
      "epoch 491: train loss = 1.61\n",
      "epoch 501: train loss = 1.61\n",
      "epoch 511: train loss = 1.61\n",
      "epoch 521: train loss = 1.61\n",
      "epoch 531: train loss = 1.61\n",
      "epoch 541: train loss = 1.6\n",
      "epoch 551: train loss = 1.6\n",
      "epoch 561: train loss = 1.6\n",
      "epoch 571: train loss = 1.6\n",
      "epoch 581: train loss = 1.6\n",
      "epoch 591: train loss = 1.6\n",
      "epoch 601: train loss = 1.6\n",
      "epoch 611: train loss = 1.6\n",
      "epoch 621: train loss = 1.6\n",
      "epoch 631: train loss = 1.6\n",
      "epoch 641: train loss = 1.6\n",
      "epoch 651: train loss = 1.6\n",
      "epoch 661: train loss = 1.6\n",
      "epoch 671: train loss = 1.6\n",
      "epoch 681: train loss = 1.6\n",
      "epoch 691: train loss = 1.6\n",
      "epoch 701: train loss = 1.6\n",
      "epoch 711: train loss = 1.6\n",
      "epoch 721: train loss = 1.6\n",
      "epoch 731: train loss = 1.6\n",
      "epoch 741: train loss = 1.6\n",
      "epoch 751: train loss = 1.6\n",
      "epoch 761: train loss = 1.6\n",
      "epoch 771: train loss = 1.6\n",
      "epoch 781: train loss = 1.6\n",
      "epoch 791: train loss = 1.59\n",
      "epoch 801: train loss = 1.59\n",
      "epoch 811: train loss = 1.59\n",
      "epoch 821: train loss = 1.59\n",
      "epoch 831: train loss = 1.59\n",
      "epoch 841: train loss = 1.59\n",
      "epoch 851: train loss = 1.59\n",
      "epoch 861: train loss = 1.59\n",
      "epoch 871: train loss = 1.59\n",
      "epoch 881: train loss = 1.58\n",
      "epoch 891: train loss = 1.58\n",
      "epoch 901: train loss = 1.58\n",
      "epoch 911: train loss = 1.58\n",
      "epoch 921: train loss = 1.58\n",
      "epoch 931: train loss = 1.58\n",
      "epoch 941: train loss = 1.57\n",
      "epoch 951: train loss = 1.57\n",
      "epoch 961: train loss = 1.57\n",
      "epoch 971: train loss = 1.57\n",
      "epoch 981: train loss = 1.56\n",
      "epoch 991: train loss = 1.56\n",
      "epoch 1001: train loss = 1.56\n",
      "epoch 1011: train loss = 1.56\n",
      "epoch 1021: train loss = 1.56\n",
      "epoch 1031: train loss = 1.55\n",
      "epoch 1041: train loss = 1.55\n",
      "epoch 1051: train loss = 1.55\n",
      "epoch 1061: train loss = 1.55\n",
      "epoch 1071: train loss = 1.54\n",
      "epoch 1081: train loss = 1.54\n",
      "epoch 1091: train loss = 1.54\n",
      "epoch 1101: train loss = 1.54\n",
      "epoch 1111: train loss = 1.53\n",
      "epoch 1121: train loss = 1.53\n",
      "epoch 1131: train loss = 1.53\n",
      "epoch 1141: train loss = 1.53\n",
      "epoch 1151: train loss = 1.53\n",
      "epoch 1161: train loss = 1.52\n",
      "epoch 1171: train loss = 1.52\n",
      "epoch 1181: train loss = 1.52\n",
      "epoch 1191: train loss = 1.52\n",
      "epoch 1201: train loss = 1.52\n",
      "epoch 1211: train loss = 1.51\n",
      "epoch 1221: train loss = 1.51\n",
      "epoch 1231: train loss = 1.51\n",
      "epoch 1241: train loss = 1.51\n",
      "epoch 1251: train loss = 1.51\n",
      "epoch 1261: train loss = 1.5\n",
      "epoch 1271: train loss = 1.5\n",
      "epoch 1281: train loss = 1.5\n",
      "epoch 1291: train loss = 1.5\n",
      "epoch 1301: train loss = 1.5\n",
      "epoch 1311: train loss = 1.49\n",
      "epoch 1321: train loss = 1.49\n",
      "epoch 1331: train loss = 1.49\n",
      "epoch 1341: train loss = 1.49\n",
      "epoch 1351: train loss = 1.48\n",
      "epoch 1361: train loss = 1.48\n",
      "epoch 1371: train loss = 1.48\n",
      "epoch 1381: train loss = 1.48\n",
      "epoch 1391: train loss = 1.48\n",
      "epoch 1401: train loss = 1.47\n",
      "epoch 1411: train loss = 1.47\n",
      "epoch 1421: train loss = 1.47\n",
      "epoch 1431: train loss = 1.47\n",
      "epoch 1441: train loss = 1.46\n",
      "epoch 1451: train loss = 1.46\n",
      "epoch 1461: train loss = 1.47\n",
      "epoch 1471: train loss = 1.46\n",
      "epoch 1481: train loss = 1.46\n",
      "epoch 1491: train loss = 1.47\n",
      "epoch 1501: train loss = 1.45\n",
      "epoch 1511: train loss = 1.46\n",
      "epoch 1521: train loss = 1.45\n",
      "epoch 1531: train loss = 1.45\n",
      "epoch 1541: train loss = 1.44\n",
      "epoch 1551: train loss = 1.45\n",
      "epoch 1561: train loss = 1.44\n",
      "epoch 1571: train loss = 1.44\n",
      "epoch 1581: train loss = 1.44\n",
      "epoch 1591: train loss = 1.43\n",
      "epoch 1601: train loss = 1.43\n",
      "epoch 1611: train loss = 1.43\n",
      "epoch 1621: train loss = 1.42\n",
      "epoch 1631: train loss = 1.42\n",
      "epoch 1641: train loss = 1.42\n",
      "epoch 1651: train loss = 1.42\n",
      "epoch 1661: train loss = 1.41\n",
      "epoch 1671: train loss = 1.41\n",
      "epoch 1681: train loss = 1.41\n",
      "epoch 1691: train loss = 1.41\n",
      "epoch 1701: train loss = 1.41\n",
      "epoch 1711: train loss = 1.4\n",
      "epoch 1721: train loss = 1.4\n",
      "epoch 1731: train loss = 1.4\n",
      "epoch 1741: train loss = 1.4\n",
      "epoch 1751: train loss = 1.39\n",
      "epoch 1761: train loss = 1.39\n",
      "epoch 1771: train loss = 1.39\n",
      "epoch 1781: train loss = 1.39\n",
      "epoch 1791: train loss = 1.39\n",
      "epoch 1801: train loss = 1.38\n",
      "epoch 1811: train loss = 1.38\n",
      "epoch 1821: train loss = 1.38\n",
      "epoch 1831: train loss = 1.38\n",
      "epoch 1841: train loss = 1.37\n",
      "epoch 1851: train loss = 1.37\n",
      "epoch 1861: train loss = 1.37\n",
      "epoch 1871: train loss = 1.37\n",
      "epoch 1881: train loss = 1.37\n",
      "epoch 1891: train loss = 1.36\n",
      "epoch 1901: train loss = 1.36\n",
      "epoch 1911: train loss = 1.36\n",
      "epoch 1921: train loss = 1.35\n",
      "epoch 1931: train loss = 1.36\n",
      "epoch 1941: train loss = 1.36\n",
      "epoch 1951: train loss = 1.36\n",
      "epoch 1961: train loss = 1.35\n",
      "epoch 1971: train loss = 1.34\n",
      "epoch 1981: train loss = 1.35\n",
      "epoch 1991: train loss = 1.34\n",
      "epoch 2001: train loss = 1.33\n",
      "epoch 2011: train loss = 1.33\n",
      "epoch 2021: train loss = 1.33\n",
      "epoch 2031: train loss = 1.33\n",
      "epoch 2041: train loss = 1.31\n",
      "epoch 2051: train loss = 1.47\n",
      "epoch 2061: train loss = 1.4\n",
      "epoch 2071: train loss = 1.37\n",
      "epoch 2081: train loss = 1.42\n",
      "epoch 2091: train loss = 1.31\n",
      "epoch 2101: train loss = 1.34\n",
      "epoch 2111: train loss = 1.3\n",
      "epoch 2121: train loss = 1.3\n",
      "epoch 2131: train loss = 1.31\n",
      "epoch 2141: train loss = 1.3\n",
      "epoch 2151: train loss = 1.31\n",
      "epoch 2161: train loss = 1.29\n",
      "epoch 2171: train loss = 1.31\n",
      "epoch 2181: train loss = 1.29\n",
      "epoch 2191: train loss = 1.29\n",
      "epoch 2201: train loss = 1.29\n",
      "epoch 2211: train loss = 1.29\n",
      "epoch 2221: train loss = 1.3\n",
      "epoch 2231: train loss = 1.28\n",
      "epoch 2241: train loss = 1.28\n",
      "epoch 2251: train loss = 1.32\n",
      "epoch 2261: train loss = 1.28\n",
      "epoch 2271: train loss = 1.27\n",
      "epoch 2281: train loss = 1.27\n",
      "epoch 2291: train loss = 1.27\n",
      "epoch 2301: train loss = 1.31\n",
      "epoch 2311: train loss = 1.3\n",
      "epoch 2321: train loss = 1.3\n",
      "epoch 2331: train loss = 1.28\n",
      "epoch 2341: train loss = 1.35\n",
      "epoch 2351: train loss = 1.28\n",
      "epoch 2361: train loss = 1.38\n",
      "epoch 2371: train loss = 1.31\n",
      "epoch 2381: train loss = 1.26\n",
      "epoch 2391: train loss = 1.25\n",
      "epoch 2401: train loss = 1.27\n",
      "epoch 2411: train loss = 1.25\n",
      "epoch 2421: train loss = 1.27\n",
      "epoch 2431: train loss = 1.26\n",
      "epoch 2441: train loss = 1.27\n",
      "epoch 2451: train loss = 1.25\n",
      "epoch 2461: train loss = 1.26\n",
      "epoch 2471: train loss = 1.25\n",
      "epoch 2481: train loss = 1.28\n",
      "epoch 2491: train loss = 1.25\n",
      "epoch 2501: train loss = 1.25\n",
      "epoch 2511: train loss = 1.25\n",
      "epoch 2521: train loss = 1.28\n",
      "epoch 2531: train loss = 1.24\n",
      "epoch 2541: train loss = 1.25\n",
      "epoch 2551: train loss = 1.24\n",
      "epoch 2561: train loss = 1.28\n",
      "epoch 2571: train loss = 1.24\n",
      "epoch 2581: train loss = 1.24\n",
      "epoch 2591: train loss = 1.23\n",
      "epoch 2601: train loss = 1.25\n",
      "epoch 2611: train loss = 1.28\n",
      "epoch 2621: train loss = 1.45\n",
      "epoch 2631: train loss = 1.32\n",
      "epoch 2641: train loss = 1.3\n",
      "epoch 2651: train loss = 1.24\n",
      "epoch 2661: train loss = 1.3\n",
      "epoch 2671: train loss = 1.22\n",
      "epoch 2681: train loss = 1.23\n",
      "epoch 2691: train loss = 1.24\n",
      "epoch 2701: train loss = 1.24\n",
      "epoch 2711: train loss = 1.23\n",
      "epoch 2721: train loss = 1.22\n",
      "epoch 2731: train loss = 1.23\n",
      "epoch 2741: train loss = 1.22\n",
      "epoch 2751: train loss = 1.23\n",
      "epoch 2761: train loss = 1.24\n",
      "epoch 2771: train loss = 1.23\n",
      "epoch 2781: train loss = 1.23\n",
      "epoch 2791: train loss = 1.22\n",
      "epoch 2801: train loss = 1.23\n",
      "epoch 2811: train loss = 1.22\n",
      "epoch 2821: train loss = 1.23\n",
      "epoch 2831: train loss = 1.23\n",
      "epoch 2841: train loss = 1.23\n",
      "epoch 2851: train loss = 1.21\n",
      "epoch 2861: train loss = 1.22\n",
      "epoch 2871: train loss = 1.22\n",
      "epoch 2881: train loss = 1.22\n",
      "epoch 2891: train loss = 1.22\n",
      "epoch 2901: train loss = 1.26\n",
      "epoch 2911: train loss = 1.32\n",
      "epoch 2921: train loss = 1.34\n",
      "epoch 2931: train loss = 1.26\n",
      "epoch 2941: train loss = 1.22\n",
      "epoch 2951: train loss = 1.23\n",
      "epoch 2961: train loss = 1.35\n",
      "epoch 2971: train loss = 1.26\n",
      "epoch 2981: train loss = 1.34\n",
      "epoch 2991: train loss = 1.28\n",
      "epoch 3001: train loss = 1.28\n",
      "epoch 3011: train loss = 1.4\n",
      "epoch 3021: train loss = 1.24\n",
      "epoch 3031: train loss = 1.23\n",
      "epoch 3041: train loss = 1.22\n",
      "epoch 3051: train loss = 1.21\n",
      "epoch 3061: train loss = 1.22\n",
      "epoch 3071: train loss = 1.22\n",
      "epoch 3081: train loss = 1.2\n",
      "epoch 3091: train loss = 1.21\n",
      "epoch 3101: train loss = 1.2\n",
      "epoch 3111: train loss = 1.2\n",
      "epoch 3121: train loss = 1.22\n",
      "epoch 3131: train loss = 1.21\n",
      "epoch 3141: train loss = 1.21\n",
      "epoch 3151: train loss = 1.23\n",
      "epoch 3161: train loss = 1.22\n",
      "epoch 3171: train loss = 1.22\n",
      "epoch 3181: train loss = 1.2\n",
      "epoch 3191: train loss = 1.21\n",
      "epoch 3201: train loss = 1.2\n",
      "epoch 3211: train loss = 1.2\n",
      "epoch 3221: train loss = 1.2\n",
      "epoch 3231: train loss = 1.2\n",
      "epoch 3241: train loss = 1.21\n",
      "epoch 3251: train loss = 1.21\n",
      "epoch 3261: train loss = 1.19\n",
      "epoch 3271: train loss = 1.23\n",
      "epoch 3281: train loss = 1.19\n",
      "epoch 3291: train loss = 1.22\n",
      "epoch 3301: train loss = 1.21\n",
      "epoch 3311: train loss = 1.19\n",
      "epoch 3321: train loss = 1.19\n",
      "epoch 3331: train loss = 1.19\n",
      "epoch 3341: train loss = 1.19\n",
      "epoch 3351: train loss = 1.19\n",
      "epoch 3361: train loss = 1.19\n",
      "epoch 3371: train loss = 1.19\n",
      "epoch 3381: train loss = 1.19\n",
      "epoch 3391: train loss = 1.19\n",
      "epoch 3401: train loss = 1.19\n",
      "epoch 3411: train loss = 1.19\n",
      "epoch 3421: train loss = 1.18\n",
      "epoch 3431: train loss = 1.18\n",
      "epoch 3441: train loss = 1.18\n",
      "epoch 3451: train loss = 1.18\n",
      "epoch 3461: train loss = 1.18\n",
      "epoch 3471: train loss = 1.18\n",
      "epoch 3481: train loss = 1.19\n",
      "epoch 3491: train loss = 1.19\n",
      "epoch 3501: train loss = 1.21\n",
      "epoch 3511: train loss = 1.18\n",
      "epoch 3521: train loss = 1.18\n",
      "epoch 3531: train loss = 1.18\n",
      "epoch 3541: train loss = 1.18\n",
      "epoch 3551: train loss = 1.18\n",
      "epoch 3561: train loss = 1.18\n",
      "epoch 3571: train loss = 1.18\n",
      "epoch 3581: train loss = 1.18\n",
      "epoch 3591: train loss = 1.18\n",
      "epoch 3601: train loss = 1.18\n",
      "epoch 3611: train loss = 1.17\n",
      "epoch 3621: train loss = 1.18\n",
      "epoch 3631: train loss = 1.17\n",
      "epoch 3641: train loss = 1.17\n",
      "epoch 3651: train loss = 1.18\n",
      "epoch 3661: train loss = 1.18\n",
      "epoch 3671: train loss = 1.17\n",
      "epoch 3681: train loss = 1.17\n",
      "epoch 3691: train loss = 1.17\n",
      "epoch 3701: train loss = 1.17\n",
      "epoch 3711: train loss = 1.17\n",
      "epoch 3721: train loss = 1.17\n",
      "epoch 3731: train loss = 1.17\n",
      "epoch 3741: train loss = 1.17\n",
      "epoch 3751: train loss = 1.17\n",
      "epoch 3761: train loss = 1.17\n",
      "epoch 3771: train loss = 1.17\n",
      "epoch 3781: train loss = 1.17\n",
      "epoch 3791: train loss = 1.17\n",
      "epoch 3801: train loss = 1.17\n",
      "epoch 3811: train loss = 1.17\n",
      "epoch 3821: train loss = 1.18\n",
      "epoch 3831: train loss = 1.31\n",
      "epoch 3841: train loss = 1.23\n",
      "epoch 3851: train loss = 1.18\n",
      "epoch 3861: train loss = 1.17\n",
      "epoch 3871: train loss = 1.17\n",
      "epoch 3881: train loss = 1.17\n",
      "epoch 3891: train loss = 1.17\n",
      "epoch 3901: train loss = 1.18\n",
      "epoch 3911: train loss = 1.17\n",
      "epoch 3921: train loss = 1.17\n",
      "epoch 3931: train loss = 1.16\n",
      "epoch 3941: train loss = 1.17\n",
      "epoch 3951: train loss = 1.17\n",
      "epoch 3961: train loss = 1.16\n",
      "epoch 3971: train loss = 1.16\n",
      "epoch 3981: train loss = 1.17\n",
      "epoch 3991: train loss = 1.16\n",
      "epoch 4001: train loss = 1.17\n",
      "epoch 4011: train loss = 1.17\n",
      "epoch 4021: train loss = 1.17\n",
      "epoch 4031: train loss = 1.16\n",
      "epoch 4041: train loss = 1.17\n",
      "epoch 4051: train loss = 1.17\n",
      "epoch 4061: train loss = 1.16\n",
      "epoch 4071: train loss = 1.16\n",
      "epoch 4081: train loss = 1.26\n",
      "epoch 4091: train loss = 1.21\n",
      "epoch 4101: train loss = 1.27\n",
      "epoch 4111: train loss = 1.19\n",
      "epoch 4121: train loss = 1.18\n",
      "epoch 4131: train loss = 1.22\n",
      "epoch 4141: train loss = 1.22\n",
      "epoch 4151: train loss = 1.18\n",
      "epoch 4161: train loss = 1.16\n",
      "epoch 4171: train loss = 1.16\n",
      "epoch 4181: train loss = 1.16\n",
      "epoch 4191: train loss = 1.16\n",
      "epoch 4201: train loss = 1.16\n",
      "epoch 4211: train loss = 1.16\n",
      "epoch 4221: train loss = 1.16\n",
      "epoch 4231: train loss = 1.16\n",
      "epoch 4241: train loss = 1.16\n",
      "epoch 4251: train loss = 1.16\n",
      "epoch 4261: train loss = 1.16\n",
      "epoch 4271: train loss = 1.16\n",
      "epoch 4281: train loss = 1.16\n",
      "epoch 4291: train loss = 1.16\n",
      "epoch 4301: train loss = 1.16\n",
      "epoch 4311: train loss = 1.16\n",
      "epoch 4321: train loss = 1.16\n",
      "epoch 4331: train loss = 1.16\n",
      "epoch 4341: train loss = 1.16\n",
      "epoch 4351: train loss = 1.15\n",
      "epoch 4361: train loss = 1.15\n",
      "epoch 4371: train loss = 1.16\n",
      "epoch 4381: train loss = 1.16\n",
      "epoch 4391: train loss = 1.15\n",
      "epoch 4401: train loss = 1.16\n",
      "epoch 4411: train loss = 1.16\n",
      "epoch 4421: train loss = 1.17\n",
      "epoch 4431: train loss = 1.16\n",
      "epoch 4441: train loss = 1.16\n",
      "epoch 4451: train loss = 1.16\n",
      "epoch 4461: train loss = 1.16\n",
      "epoch 4471: train loss = 1.16\n",
      "epoch 4481: train loss = 1.16\n",
      "epoch 4491: train loss = 1.16\n",
      "epoch 4501: train loss = 1.16\n",
      "epoch 4511: train loss = 1.15\n",
      "epoch 4521: train loss = 1.16\n",
      "epoch 4531: train loss = 1.15\n",
      "epoch 4541: train loss = 1.15\n",
      "epoch 4551: train loss = 1.16\n",
      "epoch 4561: train loss = 1.16\n",
      "epoch 4571: train loss = 1.16\n",
      "epoch 4581: train loss = 1.16\n",
      "epoch 4591: train loss = 1.15\n",
      "epoch 4601: train loss = 1.15\n",
      "epoch 4611: train loss = 1.15\n",
      "epoch 4621: train loss = 1.15\n",
      "epoch 4631: train loss = 1.15\n",
      "epoch 4641: train loss = 1.16\n",
      "epoch 4651: train loss = 1.15\n",
      "epoch 4661: train loss = 1.15\n",
      "epoch 4671: train loss = 1.16\n",
      "epoch 4681: train loss = 1.16\n",
      "epoch 4691: train loss = 1.15\n",
      "epoch 4701: train loss = 1.16\n",
      "epoch 4711: train loss = 1.15\n",
      "epoch 4721: train loss = 1.15\n",
      "epoch 4731: train loss = 1.17\n",
      "epoch 4741: train loss = 1.14\n",
      "epoch 4751: train loss = 1.15\n",
      "epoch 4761: train loss = 1.16\n",
      "epoch 4771: train loss = 1.15\n",
      "epoch 4781: train loss = 1.15\n",
      "epoch 4791: train loss = 1.15\n",
      "epoch 4801: train loss = 1.15\n",
      "epoch 4811: train loss = 1.15\n",
      "epoch 4821: train loss = 1.15\n",
      "epoch 4831: train loss = 1.15\n",
      "epoch 4841: train loss = 1.15\n",
      "epoch 4851: train loss = 1.15\n",
      "epoch 4861: train loss = 1.15\n",
      "epoch 4871: train loss = 1.15\n",
      "epoch 4881: train loss = 1.15\n",
      "epoch 4891: train loss = 1.15\n",
      "epoch 4901: train loss = 1.15\n",
      "epoch 4911: train loss = 1.15\n",
      "epoch 4921: train loss = 1.16\n",
      "epoch 4931: train loss = 1.16\n",
      "epoch 4941: train loss = 1.34\n",
      "epoch 4951: train loss = 1.15\n",
      "epoch 4961: train loss = 1.14\n",
      "epoch 4971: train loss = 1.15\n",
      "epoch 4981: train loss = 1.14\n",
      "epoch 4991: train loss = 1.14\n",
      "The model has been saved\n",
      "(1) Confusion matrix:\n",
      " [[4067  702 2796 2588  197]\n",
      " [2447 1080 2573 3933  317]\n",
      " [2702 1249 2629 3641  268]\n",
      " [2433 1040 1727 4823  477]\n",
      " [2144 1172 2450 4200  534]]\n",
      "(2) micro F1 score = 0.25\n",
      "(3) Macro F1 score = 0.22\n",
      "(4) Percentage Classification accuracy = 25.16%\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "Linear_LOP0CV(normalised_lili, normalised_dat_2022, model=Linear_all_features_properties(), num_folds=5, predicting_feature='property_id', learning_rate=0.05, num_epochs=5000, random_state=num)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384.22882103919983"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2-t1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
